{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAbyICyIP7B2"
      },
      "source": [
        "# Audio Fingerprinting for Model Attribution: A Hands-On Tutorial\n",
        "This tutorial will guide you through the process of determining whether a given audio sample was generated by a specific target neural speech synthesis system, using techniques based on audio fingerprinting.\n",
        "\n",
        "We will cover:\n",
        "1.   Loading and visualizing audio signals.\n",
        "2.   Model-specific fingerprints.\n",
        "3.   Fingerprint Extraction.\n",
        "4.   Attribution: Correlation and Mahalanobis Distance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAPBMMOtP6Av"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "This practical session uses Python, specifically Numpy, Scipy, and PyTorch, with all dependencies expected to be pre-installed in the Colab environment.\n",
        "\n",
        "The only requirement is to download a few pre-trained models and sample data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdTYyuU9lpZ-"
      },
      "outputs": [],
      "source": [
        "!echo \"Download external zip \"\n",
        "!wget --quiet --show-progress -O tutorial.zip https://www.dropbox.com/scl/fi/thzg1pxy1ke1f4onmb9r4/tutorial.zip?rlkey=2trnij764jjvun1rvlsetadf0&st=uu9j9dlk&dl=1\n",
        "!echo \"Unzip files\"\n",
        "!unzip -q -o tutorial.zip -x / -d /content\n",
        "!if [ -d \"/content/tutorial\" ]; then echo \"Done\"; else echo \"Please contact with the author\"; fi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pediAE-w01i4"
      },
      "outputs": [],
      "source": [
        "# !rm -rf /content/tutorial\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm76M-DW6mCA"
      },
      "source": [
        "Audio samples in this tutorial come from:\n",
        "\n",
        "- **WaveFake Corpus**:\n",
        " Licensed under the MIT License. © 2021 Ruhr University Bochum.  \n",
        "  Permission is granted to use, copy, modify, and distribute the software under the terms of the MIT License.  \n",
        "  Source: [WaveFake GitHub Repository](https://github.com/RUB-SysSec/WaveFake)\n",
        "- **LJ Speech Dataset**:\n",
        "  This dataset is in the public domain in the US and likely other countries.\n",
        "  There are no restrictions on its use.  \n",
        "  Source: [LJ Speech Dataset](https://keithito.com/LJ-Speech-Dataset/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obFfyyeGSCLJ"
      },
      "source": [
        "## 1. Load and Listen to Audio\n",
        "We begin processing audio, let's start by uploading a .wav file.\n",
        "This is useful if you want to test your own recordings or custom audio samples.\n",
        "\n",
        "Let’s load it and listen to it using PyTorch's torchaudio and IPython:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFWh3xxPSG-I"
      },
      "outputs": [],
      "source": [
        "import torchaudio\n",
        "import IPython.display as ipd\n",
        "\n",
        "# Loading audio file\n",
        "audio_path = 'tutorial/real/LJ001-0005.wav'\n",
        "waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "# waveform shape: [channels, samples]\n",
        "duration = waveform.shape[1] / sample_rate\n",
        "print(f\"Audio duration: {duration:.2f} seconds, Sample rate: {sample_rate} Hz\")\n",
        "\n",
        "# Listening to audio\n",
        "ipd.Audio(waveform[0].numpy(), rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYvr58kgSzOo"
      },
      "source": [
        "### Plot the Spectrogram\n",
        "A spectrogram is a visual representation of the range of frequencies in a signal as it changes over time. It is commonly used in audio processing and speech recognition to analyze frequency content and patterns, such as pitch, harmonics, or phonemes.\n",
        "\n",
        "To generate a spectrogram, we apply the Short-Time Fourier Transform (STFT) to the waveform. STFT breaks the audio into overlapping chunks (windows) and performs a Fourier transform on each, resulting in a time-frequency representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sij83itw1wxw"
      },
      "source": [
        "This code uses torchaudio and matplotlib to compute and plot spectrograms with different configurations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ahA54FaQWXh"
      },
      "outputs": [],
      "source": [
        "import torchaudio.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "def plot_spectrogram(waveform, sample_rate, n_fft=1024, hop_length=256, title=None):\n",
        "  \"\"\"\n",
        "  Compute and plot a spectrogram using torch's STFT.\n",
        "\n",
        "  Args:\n",
        "      waveform (Tensor): Audio tensor of shape [1, T] or [C, T]\n",
        "      sample_rate (int): Sampling rate of the waveform\n",
        "      n_fft (int): Size of FFT window (affects frequency resolution)\n",
        "      hop_length (int): Step size between windows (affects time resolution)\n",
        "      title (str): Optional plot title\n",
        "  \"\"\"\n",
        "\n",
        "  # Ensure mono\n",
        "  if waveform.shape[0] > 1:\n",
        "      waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "  transform = T.Spectrogram(n_fft=n_fft, hop_length=hop_length, power=2)\n",
        "  spec = transform(waveform)\n",
        "  spec_db = 10 * torch.log10(spec + 1e-10)\n",
        "\n",
        "  plt.figure(figsize=(8, 3))\n",
        "  plt.imshow(\n",
        "      spec_db[0].numpy(),\n",
        "      aspect='auto',\n",
        "      origin='lower',\n",
        "      extent=[0, waveform.shape[1] / sample_rate, 0, sample_rate // 2]\n",
        "  )\n",
        "  plt.colorbar(format='%+2.0f dB')\n",
        "  plt.title(title or f\"Spectrogram (n_fft={n_fft}, hop_length={hop_length})\")\n",
        "  plt.xlabel(\"Time (s)\")\n",
        "  plt.ylabel(\"Frequency (Hz)\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, we ensure that the audio is mono (single channel), as many audio analysis methods assume this format for simplicity and consistency.\n",
        "\n",
        "Each call to plot_spectrogram shows a different time-frequency resolution:\n",
        "\n",
        "* n_fft controls the frequency resolution. Larger values allow you to see finer frequency details.\n",
        "* hop_length controls the time resolution. Smaller values provide finer temporal details but increase computational cost.\n",
        "\n",
        "By varying these parameters, we can gain different perspectives on the spectral content of the audio — a useful step before fingerprinting.\n",
        "\n"
      ],
      "metadata": {
        "id": "5FOhvjBTHJ8H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXBbBZobS5tA"
      },
      "outputs": [],
      "source": [
        "import IPython.display as ipd\n",
        "\n",
        "# Loading audio file\n",
        "audio_path = 'tutorial/real/LJ001-0005.wav'\n",
        "waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "# Convert to mono if needed\n",
        "if waveform.shape[0] > 1:\n",
        "    waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "# n_fft: Determines the frequency resolution. Larger values capture finer frequency detail.\n",
        "# hop_length: Controls time resolution (step size between windows). Smaller values give better time granularity.\n",
        "# Example usage with your waveform\n",
        "plot_spectrogram(waveform, sample_rate, n_fft=128, hop_length=2)\n",
        "plot_spectrogram(waveform, sample_rate, n_fft=1024, hop_length=256)\n",
        "plot_spectrogram(waveform, sample_rate, n_fft=2048, hop_length=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVUfROCU3u29"
      },
      "source": [
        "When we compute a spectrogram, we are usually calculating the power (or magnitude squared) of the signal at different frequencies over time.\n",
        "However, these raw power values can vary by several orders of magnitude, making them difficult to interpret and visualize.\n",
        "To solve this, we convert the power spectrogram to a decibel (dB) scale, which compresses the dynamic range, makes the plot more visually informative, and matches how humans perceive loudness (we hear in a logarithmic scale).\n",
        "\n",
        "When plotting the spectrogram, the frequency axis (Y-axis) typically ranges from 0 Hz to half the sampling rate. This is due to the Nyquist Theorem, which states that the maximum frequency that can be represented in a digital signal is half the sample rate (sr / 2).\n",
        "This upper limit is called the Nyquist frequency. Frequencies above this are redundant due to the symmetry of the Fourier transform when applied to real-valued signals.\n",
        "For example, if your audio is sampled at 22,050 Hz, the spectrogram will only show frequencies up to 11,025 Hz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r90pQfkx51Ho"
      },
      "source": [
        " ### Try It Yourself!\n",
        " Now that you've seen how to compute and visualize spectrograms, try experimenting with different audio files!\n",
        "\n",
        "To try this with a different audio file, simply change the value of audio_path in the cell above to point to another .wav file, and then re-run the entire cell to update both the waveform and its spectrograms.\n",
        "Suggested files to try:\n",
        "* LJ001-0013.wav\n",
        "* LJ001-0036.wav\n",
        "* LJ001-0058.wav"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPVjWN0I9sO6"
      },
      "source": [
        "## 2. Model-specific fingerprints\n",
        "The concept of model-specific fingerprints was initially explored by [Marra et al.](https://arxiv.org/abs/1812.11842) in the context of image generation using Generative Adversarial Networks (GANs).   \n",
        "Their idea is based on the assumption that any artificially generated image $x$ decomposes into its content $I(x)$ and a fingerprint $F$ that is unrelated to the image semantics but specific to the model, that is,\n",
        "\\begin{equation*}\n",
        "x = I(x) + F\n",
        "\\end{equation*}\n",
        "for every $x$ generated from the model.  \n",
        "To extract the fingerprint, they assumed that a suitable image filter $f$ is capable of removing the fingerprint, such that $f(x) \\approx I(x)$, and therefore the residual\n",
        "\\begin{equation*}\n",
        "R := x - f(x) \\approx F.\n",
        "\\end{equation*}\n",
        "Given a set of generated samples $x_1, \\dots, x_N$, the sample-wise residuals are defined by $R_i:= x_i - f(x_i)$ and $F$ is estimated by\n",
        "\\begin{equation*}\n",
        "\\hat{F}:= \\frac{1}{N}\\sum_{i=1}^N R_i.\n",
        "\\end{equation*}\n",
        "For inference, i.e., for checking whether a test sample $x_{\\operatorname{test}}$ contains a fingerprint similar to $\\hat{F}$, they first compute its residual $R_{\\operatorname{test}}$ as above and then assign an attribution score $s_{\\operatorname{attr}}(R_{\\operatorname{test}}; \\, \\hat{F}) $, that could be for example the Pearson correlation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEhoOVGtCcf6"
      },
      "source": [
        "Unlike images, audio signals pose unique challenges for fingerprint extraction.\n",
        "This task presents two main challenges that distinguish audio fingerprinting from its image counterpart:\n",
        "\n",
        "a) Variable-length: Unlike fixed-size images, audio signals vary in duration, but fingerprint extraction requires a fixed-size representation for consistent analysis.\n",
        "\n",
        "b) Content-preserving filtering: While image fingerprints rely on spatial filtering, audio requires filters that suppress content-related features such as speech semantics and phonetics, while preserving subtle generative model-specific artifacts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQQf5ol5E8XF"
      },
      "source": [
        "### a) Fixed-size Representation\n",
        "To handle the variable length of audio signals, we convert each signal into a fixed-size representation that summarizes its spectral content over time.\n",
        "We apply the Short-Time Fourier Transform (STFT) to compute a spectrogram — a time-frequency representation showing how energy is distributed across frequencies and time. Then, we average the energy values across the time axis for each frequency bin. This results in a single value per frequency bin, representing its average energy over the entire duration of the audio.\n",
        "\n",
        "This fixed-size vector serves as a compact summary of the audio’s frequency profile."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNqfRybwJCfV"
      },
      "outputs": [],
      "source": [
        "import torchaudio.transforms as T\n",
        "\n",
        "audio_path = 'tutorial/real/LJ001-0005.wav'\n",
        "waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "# Compute the spectrogram\n",
        "transform = T.Spectrogram(n_fft=2048, hop_length=128)\n",
        "spec = transform(waveform)\n",
        "spec = 10. * torch.log(spec + 10e-13)\n",
        "print(\"Spectrogram shape:\", spec.shape)\n",
        "# Average over time axis (dim=2) to get a single vector per channel\n",
        "bin_data = torch.nanmean(spec, dim=2)\n",
        "print(\"Averaged bin data shape:\", bin_data.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVN1nilPLMRh"
      },
      "source": [
        "Try analyzing a different audio file, such as LJ001-0013.wav, which has a different duration but the same sampling rate.\n",
        "\n",
        "You'll notice that the time dimension of the spectrogram changes with the audio length — longer audio results in more time frames.\n",
        "\n",
        "However, the number of frequency bins remains constant, because it is determined by the n_fft parameter, which controls the frequency resolution and is independent of the audio duration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou7gya-XTDDv"
      },
      "source": [
        "### b) Suitable Filtering Methods\n",
        "To isolate generative artifacts, we apply content-preserving filters to the audio signal before computing the residuals of our spectral energy representation.\n",
        "We investigate two types of filtering approaches: (1) neural compression using Encodec, and (2) low-pass filtering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD5cRgUMNNr9"
      },
      "source": [
        "**EnCodec:** It is a machine learning model designed to compress and decompress audio — much like how ZIP files or MP3s work, but using deep learning.\n",
        "It does this by learning how to represent audio using a much smaller set of data (a compressed version) without losing too much quality. This compressed version is called a latent representation. EnCodec can then reconstruct the original audio from this compressed form."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUIULAO31YoS"
      },
      "source": [
        "Run the following cell before importing anything from EnCodec:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yivH7U3S0Lty"
      },
      "outputs": [],
      "source": [
        "!pip install -q encodec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The apply_encodec function compresses and decompresses an audio waveform using EnCodec neural audio codec at a specified bitrate (default 24 kbps). It ensures the input is mono, resamples it to 24 kHz (as required by the model), and passes it through the codec to simulate reconstruction after compression. The output is then resampled back to the original sample rate if necessary. This process helps evaluate how EnCodec alters the audio signal, which is useful in tasks like fingerprinting or model attribution where subtle differences in audio may be significant."
      ],
      "metadata": {
        "id": "bKH3L5QVK2Pq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tUrczOl95jfJ"
      },
      "outputs": [],
      "source": [
        "from encodec import EncodecModel\n",
        "\n",
        "def apply_encodec(waveform, sample_rate, target_bitrate_kbps=24):\n",
        "    \"\"\"\n",
        "    Compress and decompress a waveform using EnCodec (24kHz mono).\n",
        "\n",
        "    Args:\n",
        "        waveform (Tensor): Input waveform of shape [1, T] or [C, T] where C=1.\n",
        "        sample_rate (int): Sample rate of the input waveform.\n",
        "        target_bitrate_kbps (int): Target bitrate in kbps (default is 24).\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Reconstructed waveform at the original sample rate.\n",
        "    \"\"\"\n",
        "    # Ensure waveform is mono\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "    # Load EnCodec model\n",
        "    model = EncodecModel.encodec_model_24khz()\n",
        "    model.set_target_bandwidth(target_bitrate_kbps)\n",
        "    model.eval()\n",
        "\n",
        "    # Resample to 24 kHz\n",
        "    if sample_rate != model.sample_rate:\n",
        "        resample_to_24k = torchaudio.transforms.Resample(sample_rate, model.sample_rate)\n",
        "        waveform = resample_to_24k(waveform)\n",
        "\n",
        "    # Prepare for model input\n",
        "    waveform = waveform.unsqueeze(0)  # Shape: [1, 1, T]\n",
        "\n",
        "    # Encode and decode (no gradients needed)\n",
        "    with torch.no_grad():\n",
        "        reconstructed = model(waveform)\n",
        "\n",
        "    # Remove batch dimension\n",
        "    reconstructed = reconstructed.squeeze(0)\n",
        "\n",
        "    # Resample back to original sample rate\n",
        "    if model.sample_rate != sample_rate:\n",
        "        resample_to_orig = torchaudio.transforms.Resample(model.sample_rate, sample_rate)\n",
        "        reconstructed = resample_to_orig(reconstructed)\n",
        "\n",
        "    return reconstructed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQZxh-_tmRlU"
      },
      "source": [
        "Let's compare the frequency content of the original and the processed (e.g., compressed or resynthesized) audio signals using spectrograms.\n",
        "\n",
        "What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCQT1jss5rCK",
        "outputId": "28816382-552d-4d71-85de-832afeac66a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/encodec/v0/encodec_24khz-d7cc33bc.th\" to /root/.cache/torch/hub/checkpoints/encodec_24khz-d7cc33bc.th\n",
            "100%|██████████| 88.9M/88.9M [00:01<00:00, 54.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import display, Audio\n",
        "\n",
        "# Load audio\n",
        "waveform, sample_rate = torchaudio.load('tutorial/real/LJ001-0005.wav')\n",
        "\n",
        "# Apply EnCodec compression-decompression\n",
        "output = apply_encodec(waveform, sample_rate)\n",
        "\n",
        "# Plot or listen\n",
        "plot_spectrogram(waveform, sample_rate, n_fft=2048, hop_length=512, title=\"Original audio signal\")\n",
        "display(Audio(waveform[0].numpy(), rate=sample_rate))\n",
        "\n",
        "plot_spectrogram(output, sample_rate, n_fft=2048, hop_length=512, title=\"Compressed version\")\n",
        "display(Audio(output[0].numpy(), rate=sample_rate))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlAg0XlRSE2F"
      },
      "source": [
        "**Low-pass filter:**\n",
        "This filter method retains only the low-frequency content below the cutoff, potentially emphasizing system-specific spectral characteristics in the lower bands.\n",
        "We evaluate the filters using cutoff frequency of 1 kHz, with a stopband frequency at 1.5 kHz.\n",
        "\n",
        "This filter operates by convolving the input signal with a set of learned or designed filter coefficients.\n",
        "Given a discrete-time input signal $ x^{(i)} = \\{x^{(i)}_1, x^{(i)}_2, \\dots, x^{(i)}_s\\} $, the filtered output $ y^{(i)} = \\{y^{(i)}_1, y^{(i)}_2, \\dots, y^{(i)}_s\\} $ is obtained via convolution:\n",
        "\\begin{equation*}\n",
        "y^{(i)}_n = \\sum_{k=0}^{K-1} h_k \\cdot x^{(i)}_{n-k}, \\quad \\text{for } n = 1, \\dots, s \\enspace ,\n",
        "\\end{equation*}\n",
        "where $ h_k $ are the filter coefficients, and $ K $ is the filter order, i.e., the number of coefficients in the filter.\n",
        "This corresponds to a standard 1D convolution operation with one input and one output channel.\n",
        "Reflect padding is applied to ensure the output has the same length as the input, reducing boundary artifacts."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The apply_fir_filter function applies a Finite Impulse Response (FIR) filter to a mono audio waveform using 1D convolution. It first checks that the input has the correct shape [1, N], then creates a convolution kernel from the provided filter coefficients. To preserve the original waveform length after filtering, it pads the signal using reflection padding. The function then performs the convolution using PyTorch’s conv1d, returning the filtered waveform with the same shape as the input."
      ],
      "metadata": {
        "id": "Ds-_fKLxMHyH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATaTGMbH3th7"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def apply_fir_filter(waveform: torch.Tensor, coeffs: list[float]) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Apply a FIR filter to an audio waveform using 1D convolution.\n",
        "\n",
        "    Args:\n",
        "        waveform (Tensor): Shape (1, N) for mono waveform.\n",
        "        coeffs (list of float): FIR filter coefficients.\n",
        "\n",
        "    Returns:\n",
        "        filtered waveform (Tensor): Shape (1, N) after filtering.\n",
        "    \"\"\"\n",
        "    if waveform.ndim != 2 or waveform.shape[0] != 1:\n",
        "        raise ValueError(\"Waveform must be mono (shape [1, N])\")\n",
        "\n",
        "    # Prepare kernel\n",
        "    fir_kernel = torch.tensor(coeffs, dtype=torch.float32).view(1, 1, -1)\n",
        "    kernel_size = fir_kernel.shape[2]\n",
        "\n",
        "    # Pad to maintain output size\n",
        "    pad = kernel_size // 2\n",
        "    padded = F.pad(waveform, (pad, pad), mode='reflect')\n",
        "\n",
        "    # Apply filter\n",
        "    filtered = F.conv1d(padded.unsqueeze(0), fir_kernel).squeeze(0)\n",
        "\n",
        "    return filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code loads FIR filter coefficients from a text file and applies them to an audio waveform using the apply_fir_filter function. It first reads the coefficients from lpf_1khz.txt, loads a mono version of the audio file LJ001-0005.wav, and then filters the signal to attenuate certain frequency components based on the FIR design. After filtering, it visualizes the result as a spectrogram and plays back the processed audio so you can both see and hear the effect of the filtering."
      ],
      "metadata": {
        "id": "ORqKKNT8MWp1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZeIAFMq391S"
      },
      "outputs": [],
      "source": [
        "# Load coefficients\n",
        "with open('tutorial/fingerprints/low_pass_filter/lpf_1khz.txt', 'r') as f:\n",
        "    coeffs = [float(line.strip()) for line in f if line.strip()]\n",
        "\n",
        "# Load waveform\n",
        "waveform, sample_rate = torchaudio.load('tutorial/real/LJ001-0005.wav')\n",
        "if waveform.shape[0] > 1:\n",
        "    waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "# Apply filter\n",
        "filtered_waveform = apply_fir_filter(waveform, coeffs)\n",
        "\n",
        "# Visualize and listen\n",
        "plot_spectrogram(filtered_waveform, sample_rate, n_fft=2048, hop_length=512)\n",
        "ipd.Audio(filtered_waveform.numpy(), rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-M1kCDj5OkL"
      },
      "source": [
        "What differences do you observe in the spectrogram before and after filtering?\n",
        "\n",
        "How has the filter affected the high-frequency content of the signal?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2dKtikH6N_D"
      },
      "source": [
        "## 3. Fingerprint Extraction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koz9hCbH7Ckb"
      },
      "source": [
        "As an example, we will calculate an ***ideal*** residual by comparing the original real speech with its AI-generated counterpart and visualize how the residuals differ depending on the model used."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function computes the difference in average spectral energy between an original and an AI-generated audio file by first loading and converting both to mono, then verifying their sample rates match. It computes spectrograms for each using the specified FFT and hop length parameters, converts them to decibel scale, and averages the spectral energy over time for each frequency bin. Finally, it returns the sample rate along with the element-wise difference between the original and AI-generated average spectral energies, highlighting frequency-based residual differences between the two signals."
      ],
      "metadata": {
        "id": "-4Fe_nxeNDlK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GP2Ibh387x-h"
      },
      "outputs": [],
      "source": [
        "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
        "import numpy as np\n",
        "\n",
        "def compute_bin_residual(original_audio_path, ai_audio_generated_path, n_fft=2048, hop_length=128):\n",
        "    waveform_orig, sr_orig = torchaudio.load(original_audio_path)\n",
        "    waveform_ai, sr_ai = torchaudio.load(ai_audio_generated_path)\n",
        "\n",
        "    if waveform_orig.shape[0] > 1:\n",
        "        waveform_orig = waveform_orig.mean(dim=0, keepdim=True)\n",
        "    if waveform_ai.shape[0] > 1:\n",
        "        waveform_ai = waveform_ai.mean(dim=0, keepdim=True)\n",
        "\n",
        "    if sr_orig != sr_ai:\n",
        "        raise ValueError(\"Sample rates do not match.\")\n",
        "\n",
        "    transform = T.Spectrogram(n_fft=n_fft, hop_length=hop_length)\n",
        "    spec_orig = transform(waveform_orig)\n",
        "    spec_ai = transform(waveform_ai)\n",
        "\n",
        "    spec_orig_db = 10. * torch.log10(spec_orig + 1e-10)\n",
        "    spec_ai_db = 10. * torch.log10(spec_ai + 1e-10)\n",
        "\n",
        "    bin_data_orig = torch.nanmean(spec_orig_db, dim=2).squeeze(0).numpy()\n",
        "    bin_data_ai = torch.nanmean(spec_ai_db, dim=2).squeeze(0).numpy()\n",
        "\n",
        "    return sr_orig, bin_data_orig - bin_data_ai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function plot_all_residuals takes pairs of original and AI-generated audio files, computes the spectral residuals for each pair using compute_bin_residual, and plots them in a 2x2 grid of bar charts. Each subplot shows the difference in average spectral energy across frequency bins (in kHz) between the original and AI audio generated, with consistent y-axis limits for easy comparison. The x-axis is labeled with fixed frequency ticks, and each plot is titled according to the provided labels, allowing visual analysis of frequency-specific discrepancies across multiple audio samples."
      ],
      "metadata": {
        "id": "IbQNa_ywNUuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_all_residuals(audio_pairs, titles, n_fft=2048, hop_length=128):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    residuals = []\n",
        "    sample_rates = []\n",
        "\n",
        "    # Compute all residuals first\n",
        "    for orig, ai in audio_pairs:\n",
        "        sr, residual = compute_bin_residual(orig, ai, n_fft, hop_length)\n",
        "        residuals.append(residual)\n",
        "        sample_rates.append(sr)\n",
        "\n",
        "    # Ensure all plots have same y-axis range\n",
        "    all_values = np.concatenate(residuals)\n",
        "    y_min, y_max = all_values.min(), all_values.max()\n",
        "\n",
        "    for i, ax in enumerate(axes):\n",
        "        freqs = np.linspace(0, sample_rates[i] / 2, len(residuals[i])) / 1000  # kHz\n",
        "\n",
        "        tick_positions = np.arange(0, sample_rates[i] / 2000 + 0.1, 2)\n",
        "        ax.xaxis.set_major_locator(FixedLocator(tick_positions))\n",
        "        ax.xaxis.set_major_formatter(FixedFormatter([f\"{x:.0f}\" for x in tick_positions]))\n",
        "\n",
        "        ax.bar(x=freqs, height=residuals[i], width=freqs[1]-freqs[0], color=\"#2D5B68\")\n",
        "        ax.set_ylim(y_min, y_max)\n",
        "\n",
        "        ax.set_title(titles[i], fontsize=12)\n",
        "        ax.set_xlabel(\"Frequency (kHz)\", fontsize=10)\n",
        "        ax.set_ylabel(\"Residual Energy (dB)\", fontsize=10)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=9)\n",
        "\n",
        "    fig.tight_layout(pad=2.0)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "qwndAKfyM9ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example defines four pairs of audio files, each consisting of an original real audio sample and its corresponding AI-generated version from different models. The plot_all_residuals function is then called with these pairs and titles to visualize and compare the spectral residuals—differences in frequency content—between the real audio and each AI-generated version, helping to assess how each model’s output differs from the original."
      ],
      "metadata": {
        "id": "u1giEsU_NlXI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhMUjvaJ827i"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "audio_pairs = [\n",
        "    ('tutorial/real/LJ001-0005.wav', 'tutorial/avocodo/LJ001-0005.wav'),\n",
        "    ('tutorial/real/LJ001-0005.wav', 'tutorial/bigvgan/LJ001-0005.wav'),\n",
        "    ('tutorial/real/LJ001-0005.wav', 'tutorial/hifiGAN/LJ001-0005.wav'),\n",
        "    ('tutorial/real/LJ001-0005.wav', 'tutorial/waveglow/LJ001-0005.wav'),\n",
        "]\n",
        "titles = ['Avocodo', 'BigVGAN', 'HiFi-GAN', 'WaveGlow']\n",
        "\n",
        "plot_all_residuals(audio_pairs, titles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er1p6ktYcER5"
      },
      "source": [
        "* What unique spectral fingerprints or residual patterns does each AI-generated model (e.g., Avocodo, BigVGAN, HiFi-GAN, WaveGlow) leave behind compared to real speech?\n",
        "\n",
        "* Are there specific frequency bands where differences are more pronounced for certain models?\n",
        "\n",
        "* Can these residual patterns be reliably used to identify which model generated a given audio sample?\n",
        "\n",
        "Try experimenting with different audio files such as LJ001-0013.wav, LJ001-0036.wav, and LJ001-0058.wav to explore these questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grL8EcOrdJMB"
      },
      "source": [
        "## Residual-Based Fingerprinting of AI-Generated Speech in Realistic Settings\n",
        "In real-world scenarios, we often do not have access to the original (real) audio that an AI model used as input to generate speech.\n",
        "This poses a challenge when trying to detect or attribute synthesized audio to a specific model.\n",
        "\n",
        "To address this, we instead apply a filter—such as a low-pass filter or a learned model like EnCodec—to the AI-generated audio.\n",
        "We then apply the Short-Time Fourier Transform (STFT) and compute the average energy per frequency bin for both the given audio signal and its filtered counterpart.\n",
        "Finally, we compute the residual, which serves as a proxy for the model's fingerprint.\n",
        "\n",
        "We estimate the generative model’s fingerprint $ \\hat{\\mathcal{F}} $ by averaging the residuals between the training audio samples and their filtered versions:\n",
        "\\begin{equation}\n",
        "\\hat{F} := \\frac{1}{N} \\sum_{i=1}^N \\mathcal{R}^{(i)} \\enspace , \\quad \\text{where } \\mathcal{R}^{(i)} := E_{x^{(i)}} - E_{f(x^{(i)})} \\enspace .\n",
        "\\end{equation}\n",
        "\n",
        "Here, $ f(\\cdot) $ denotes the chosen filter (e.g., EnCodec or low pass filter), $ E $ summarizes the spectral energy distribution across time, and $ \\mathcal{R}^{(i)} $ is the residual vector for signal $ x^{(i)} $."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXfUdeIofxZY"
      },
      "source": [
        "Example: Residual Calculation using EnCodec Filtering\n",
        "\n",
        "This function computes the residual spectral energy between an original waveform and its filtered version, which serves as a signature of how a given filtering process (like EnCodec compression or low-pass filtering) alters the signal. It first applies the specified filter function—either a neural codec (e.g., EnCodec) or a classical filter—then computes spectrograms for both the original and filtered audio. By converting the spectrograms to decibel scale and averaging the difference across the time axis, it produces a single vector representing the average energy discrepancy per frequency bin, capturing how much spectral information is lost or changed by the filtering process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtiFidRxfxlr"
      },
      "outputs": [],
      "source": [
        "# Load pretrained 24kHz mono EnCodec model\n",
        "\n",
        "def compute_residual(waveform, filter_function, filter_fn_param=None, n_fft=2048, hop_length=128):\n",
        "\n",
        "    # Apply filter function\n",
        "    output = filter_function(waveform, filter_fn_param)\n",
        "\n",
        "    # Compute spectrograms\n",
        "    spec_fn = T.Spectrogram(n_fft=n_fft, hop_length=hop_length)\n",
        "    spec_orig = spec_fn(waveform)\n",
        "    spec_recon = spec_fn(output)\n",
        "\n",
        "    # Convert to dB\n",
        "    spec_orig_db = 10. * torch.log10(spec_orig + 1e-10)\n",
        "    spec_recon_db = 10. * torch.log10(spec_recon + 1e-10)\n",
        "\n",
        "    E_residual = spec_orig_db - spec_recon_db\n",
        "    # Average over time\n",
        "    residual = torch.nanmean(E_residual, dim=2).squeeze(0).numpy()\n",
        "\n",
        "    return residual"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code computes and visualizes the spectral differences between an AI-generated audio sample and its EnCodec-compressed version, highlighting the frequency bands most affected by the model."
      ],
      "metadata": {
        "id": "Z9Ets0Dpzn1z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3BAstpmgUQq"
      },
      "outputs": [],
      "source": [
        "# Load an AI-generated audio\n",
        "waveform, sr = torchaudio.load('tutorial/bigvgan/LJ001-0005.wav')\n",
        "\n",
        "# Example usage\n",
        "residual = compute_residual(waveform, apply_encodec, sr, n_fft=2048, hop_length=128)\n",
        "\n",
        "# Plot\n",
        "freqs = np.linspace(0, sr // 2, len(residual)) / 1000  # in kHz\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.bar(freqs, residual, width=freqs[1]-freqs[0], color=\"#2D5B68\")\n",
        "plt.xlabel(\"Frequency (kHz)\")\n",
        "plt.ylabel(\"Residual Energy (dB)\")\n",
        "plt.title(\"Residual after EnCodec Filtering\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtW7H-6Ph4xh"
      },
      "source": [
        "#### Exercise: Residual Estimation via Low-Pass Filtering.\n",
        "\n",
        "As an exercise, replicate the residual estimation process using a low-pass filter function instead of a learned codec like EnCodec. Compute log-scale spectrograms of both the given AI-generated audio signal and its filtered counterpart using n_fft=128 and hop_length=2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_feGil3XiTcA"
      },
      "outputs": [],
      "source": [
        "# Load an AI-generated audio\n",
        "\n",
        "# Example usage\n",
        "\n",
        "# Plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx6R9uDkUJDF"
      },
      "source": [
        "## 4. Attribution: Correlation and Mahalanobis Distance\n",
        "Constructing a reliable fingerprint associated with an AI-generated model requires many audio samples.\n",
        "This is because individual residuals can vary due to content or noise, so we average residuals over multiple samples to obtain a stable representation of the model’s behavior.\n",
        "\n",
        "To simplify this tutorial, we will use precomputed (pre-trained) fingerprints for four generative models, allowing us to focus on the attribution logic rather than fingerprint estimation.\n",
        "\n",
        "### How do we attribute a test sample?\n",
        "Once fingerprints are available, we want to determine which model most likely generated a given test residual $ \\mathcal{R}_{\\mathrm{test}} $.\n",
        "\n",
        "Two common approaches are:\n",
        "\n",
        "1. Pearson Correlation:\n",
        "This method measures the alignment between the test residual vector and each fingerprint:\n",
        "\\begin{equation*}\n",
        "s_{\\operatorname{cor}}(\\mathcal{R}_{\\operatorname{test}}; \\, \\hat{F}) := \\langle \\tilde{R}_{\\operatorname{test}} \\, , \\, \\tilde{F}  \\rangle \\in [-1, 1] \\enspace ,\n",
        "\\end{equation*}\n",
        "where $\\tilde{R}_{\\operatorname{test}}$ and $\\tilde{F}$ denote the zero-mean and unit-norm versions of $R_{\\operatorname{test}}$ and $\\hat{F}$, respectively.\n",
        "\n",
        "* A higher score means the test residual is more similar in shape to the fingerprint.\n",
        "* Attribution is done by selecting the fingerprint with the highest correlation.\n",
        "\n",
        "2. Mahalanobis Distance:\n",
        "This method takes into account not just the difference but also the variance structure of the fingerprint distribution:\n",
        "\\begin{equation*}\n",
        "s_{\\operatorname{md}}(\\mathcal{R}_{\\mathrm{test}}, \\hat{F}) := \\sqrt{ (\\mathcal{R}_{\\mathrm{test}} - \\hat{F} )^\\top \\Sigma^{-1} (\\mathcal{R}_{\\mathrm{test}} - \\hat{F} ) } \\enspace ,\n",
        "\\end{equation*}\n",
        "where $ \\Sigma^{-1} $ is the inverse covariance matrix of the training residuals.\n",
        "This distance accounts for feature variance and correlation, enabling more robust attribution.\n",
        "* A lower distance indicates a closer match.\n",
        "* Attribution is done by selecting the fingerprint with the lowest Mahalanobis distance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2sCuEsgsfi5"
      },
      "source": [
        "#### Model attribution example\n",
        "Suppose we have fingerprints for four AI-generated models such as Avocodo, BigVGAN, HiFi-GAN, and WaveGlow. Let's attribute a test sample to the most likely model based on the correlation score."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, construct a dictionary that maps model names to the paths of their corresponding fingerprint files, depending on the specified filtering method—either \"encodec\" or \"low_pass_filter\". For EnCodec, it associates each model with a single .pickle file containing its fingerprint. For the low-pass filter method, it associates each model with two files: one containing the fingerprint and another with the inverse covariance matrix used for Mahalanobis distance scoring. This organized structure enables efficient lookup of model-specific fingerprint data for attribution tasks."
      ],
      "metadata": {
        "id": "i0XMZg2H0mkL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xp_DGzRk6ws5"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict\n",
        "\n",
        "def generate_fingerprint_dict(filtering_function: str, model_list: List[str]) -> Dict[str, Dict[str, str] | str]:\n",
        "    base_dir = \"tutorial/fingerprints\"\n",
        "    result = {}\n",
        "\n",
        "    if filtering_function == \"encodec\":\n",
        "        for model in model_list:\n",
        "            path = f\"{base_dir}/encodec/param=24.0_score=correlation_nfft=2048_hoplen=128_{model}.pickle\"\n",
        "            result[model] = path\n",
        "\n",
        "    elif filtering_function == \"low_pass_filter\":\n",
        "        for model in model_list:\n",
        "            fingerprint_path = f\"{base_dir}/low_pass_filter/param=1_score=mahalanobis_nfft=128_hoplen=2_fingerprint_{model}.pickle\"\n",
        "            invcov_path = f\"{base_dir}/low_pass_filter/param=1_score=mahalanobis_nfft=128_hoplen=2_invcov_{model}.pickle\"\n",
        "            result[model] = {\n",
        "                \"fingerprint\": fingerprint_path,\n",
        "                \"invcov\": invcov_path\n",
        "            }\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown filtering function: {filtering_function}\")\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7s4-ajm27hjx"
      },
      "outputs": [],
      "source": [
        "model_list = ['avocodo', 'bigvgan', 'hifigan', 'waveglow']\n",
        "\n",
        "encodec_dict = generate_fingerprint_dict(\"encodec\", model_list)\n",
        "print(\"Encodec Fingerprint Paths:\")\n",
        "for model, path in encodec_dict.items():\n",
        "    print(f\"{model}: {path}\")\n",
        "\n",
        "lowpass_dict = generate_fingerprint_dict(\"low_pass_filter\", model_list)\n",
        "print(\"\\nLow Pass Filter Fingerprint Paths:\")\n",
        "for model, paths in lowpass_dict.items():\n",
        "    print(f\"{model}:\")\n",
        "    print(f\"  Fingerprint: {paths['fingerprint']}\")\n",
        "    print(f\"  Invcov:      {paths['invcov']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second, attribution functions are defined to quantitatively compare the residual representation of a test audio sample against target model fingerprints. The correlation_score function measures similarity by computing the inner product between the residual and the fingerprint—ideal for EnCodec-based attribution. The mahalanobis_distance function computes the statistical distance between the residual and the fingerprint using a precomputed inverse covariance matrix, suitable for low-pass filter attribution. Additionally, the zero_mean_unit_norm function is used to normalize tensors by centering and scaling them, which helps ensure consistency and comparability across residual vectors."
      ],
      "metadata": {
        "id": "soquHeOR1Ve1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3dbYG-9iN4b"
      },
      "outputs": [],
      "source": [
        "# === Scoring Functions ===\n",
        "def correlation_score(fingerprint, input_residual):\n",
        "    fingerprint = fingerprint.squeeze(0)\n",
        "    return torch.inner(input_residual.cpu(), fingerprint.cpu())\n",
        "\n",
        "def mahalanobis_distance(fingerprint, residual, invcov):\n",
        "    delta = residual.cpu().flatten() - fingerprint.cpu().flatten()\n",
        "    distance = torch.sqrt(torch.dot(delta, torch.matmul(invcov.cpu(), delta)))\n",
        "    return distance\n",
        "\n",
        "def zero_mean_unit_norm(array: torch.tensor) -> torch.tensor:\n",
        "      array = array - torch.mean(array)\n",
        "      return array / torch.norm(array)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Third, the attribute_sample function performs the core attribution step by comparing the residual fingerprint of a test audio sample against fingerprints of various known generative models. It first loads the test waveform and then, for each model, computes a residual representation using either the EnCodec or low-pass filter method—depending on the filter_param. For EnCodec, it uses correlation scoring after normalizing both the residual and fingerprint; for low-pass filtering, it uses the Mahalanobis distance based on precomputed fingerprints and inverse covariance matrices. The function calculates and returns attribution scores for each candidate model, showing how likely it is that each model generated the input audio."
      ],
      "metadata": {
        "id": "1nhww9GG10Yl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS3MfuASuhrp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "def attribute_sample(\n",
        "    test_audio_path: str,\n",
        "    fingerprint_dict: dict,\n",
        "    filter_param: str,\n",
        "    coeffs = None\n",
        "):\n",
        "    \"\"\"\n",
        "    test_audio_path: path to the WAV file to process\n",
        "    fingerprint_dict: dictionary structured as:\n",
        "        - encodec: { model: path_str }\n",
        "        - low_pass_filter: { model: { \"fingerprint\": path_str, \"invcov\": path_str } }\n",
        "    filter_param: \"encodec\" or \"low_pass_filter\"\n",
        "    sr: sample rate (only for encodec)\n",
        "    coeffs: FIR coefficients (only for low_pass_filter)\n",
        "    \"\"\"\n",
        "    scores = {}\n",
        "    waveform, sr = torchaudio.load(test_audio_path)\n",
        "\n",
        "    for model, data in fingerprint_dict.items():\n",
        "        # --- ENCODEC ---\n",
        "        if filter_param == \"encodec\":\n",
        "            path_fp = data  # string\n",
        "\n",
        "            with open(path_fp, 'rb') as f:\n",
        "                fingerprint = pickle.load(f)\n",
        "\n",
        "            # Residual for encodec\n",
        "            residual = compute_residual(\n",
        "                waveform,\n",
        "                apply_encodec,\n",
        "                sr,\n",
        "                n_fft=2048,\n",
        "                hop_length=128\n",
        "            )\n",
        "            residual = torch.from_numpy(residual)\n",
        "            # Normalizing\n",
        "            residual = zero_mean_unit_norm(residual)\n",
        "            fingerprint = zero_mean_unit_norm(fingerprint)\n",
        "\n",
        "            score = correlation_score(fingerprint, residual)\n",
        "\n",
        "        # --- LOW PASS FILTER ---\n",
        "        elif filter_param == \"low_pass_filter\":\n",
        "            path_fp   = data[\"fingerprint\"]\n",
        "            path_icov = data[\"invcov\"]\n",
        "            print(f\"Fingerprint path: {path_fp}\")\n",
        "            print(f\"Inverse covariance path: {path_icov}\")\n",
        "            with open(path_fp, 'rb') as f:\n",
        "                fingerprint = pickle.load(f)\n",
        "            with open(path_icov, 'rb') as f:\n",
        "                invcov = pickle.load(f)\n",
        "\n",
        "            residual = compute_residual(\n",
        "                waveform,\n",
        "                apply_fir_filter,\n",
        "                coeffs,\n",
        "                n_fft=128,\n",
        "                hop_length=2\n",
        "            )\n",
        "            residual = torch.from_numpy(residual)\n",
        "            # Note: assuming zero_mean_unit_norm does not apply here\n",
        "            score = mahalanobis_distance(fingerprint, residual, invcov)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid filter parameter: {filter_param!r}\")\n",
        "\n",
        "        scores[model] = score.item()\n",
        "        print(f\"Score for {model}: {scores[model]:.4f}\")\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example demonstrates how to generate a dictionary of EnCodec fingerprint file paths for several models as we did before, and then compute attribution scores for a test audio sample using the EnCodec filtering method, identifying which model most likely generated the sample."
      ],
      "metadata": {
        "id": "-G9TSUZb2oKC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwnDdDiRull6"
      },
      "outputs": [],
      "source": [
        "# === Example Usage ===\n",
        "test_sample = 'tutorial/avocodo/LJ001-0005.wav'\n",
        "model_list = ['avocodo', 'bigvgan', 'hifiGAN', 'waveglow']\n",
        "filter_param = \"encodec\"\n",
        "\n",
        "filter_opt_dict = generate_fingerprint_dict(\"encodec\", model_list)\n",
        "for model, path in filter_opt_dict.items():\n",
        "    print(f\"{model}: {path}\")\n",
        "\n",
        "print(f\"Test sample generated using {os.path.basename(os.path.dirname(test_sample))} Model\")\n",
        "scores = attribute_sample(test_sample, filter_opt_dict, filter_param)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You get a score for each model—so, which model most likely generated the test audio based on these scores?"
      ],
      "metadata": {
        "id": "61_guTbo2-yL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this task, change the test_sample variable to use an audio file generated by a model other than Avocodo (e.g., BigVGAN, HiFi-GAN, or WaveGlow), and make sure to use the low-pass filter fingerprint corresponding to the Avocodo model. Your goal is to observe whether the Mahalanobis distance is lowest when the test sample actually comes from Avocodo. Repeat this process with different test samples to assess whether the low-pass filter fingerprint can reliably attribute audio to the correct generative model."
      ],
      "metadata": {
        "id": "pcXI47XO46iM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCRKuLmgfOtX"
      },
      "outputs": [],
      "source": [
        "test_sample = 'tutorial/avocodo/LJ001-0005.wav'\n",
        "model_list = ['avocodo']\n",
        "\n",
        "filter_opt_dict = generate_fingerprint_dict(\"low_pass_filter\", model_list)\n",
        "print(\"\\nLow Pass Filter Paths:\")\n",
        "\n",
        "for model, paths in filter_opt_dict.items():\n",
        "    print(f\"{model}:\")\n",
        "    print(f\"  Fingerprint: {paths['fingerprint']}\")\n",
        "    print(f\"  Invcov:      {paths['invcov']}\")\n",
        "\n",
        "print(f\"Test sample generated using {os.path.basename(os.path.dirname(test_sample))} Model\")\n",
        "# scores = attribute_sample(...)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eut3nVrDUTxk"
      },
      "source": [
        "## 5. Summary\n",
        "\n",
        "* You learned how to load, listen, and visualize audio.\n",
        "* You applied both classical and neural filters.\n",
        "* You extracted average spectral energy and computed fingerprints.\n",
        "* You performed model attribution using correlation and Mahalanobis distance."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can delete all pre-trained models and sample data by running:"
      ],
      "metadata": {
        "id": "77kS9PEY5x0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/tutorial"
      ],
      "metadata": {
        "id": "OLje71Ne54Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unekFNIVUc7e"
      },
      "source": [
        "## 6. (Optional) Extend to Your Dataset and Models\n",
        "\n",
        "* Replace the sample audio with your own dataset.\n",
        "* Use your own model outputs for real/fake audio.\n",
        "* Save and load fingerprints for attribution tasks."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}