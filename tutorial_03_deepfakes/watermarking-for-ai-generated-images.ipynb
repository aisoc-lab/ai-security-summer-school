{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3264ff37",
      "metadata": {
        "id": "3264ff37"
      },
      "source": [
        "# Watermarking for AI-Generated Images\n",
        "\n",
        "Besides passive detection techniques, providers of generative AI like Google and Meta employ watermarking to help detect their generated content.\n",
        "\n",
        "The most straight forward way to do this, is to generate content and then add an invisible signal made out of very subtle pixel perturbations and have a detector with is very sensitive to this signal. This is called post-hoc watermarking.\n",
        "\n",
        "In this assignment, we will take a closer look at such watermarking techniques.\n",
        "\n",
        "**Note:** If your local setup does not have a GPU, it is highly recommended to run this notebook on Google Colab (see this repository's README for instructions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y7Zcf7buMyLC",
      "metadata": {
        "id": "Y7Zcf7buMyLC"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    os.system('pip install opencv-python')\n",
        "    os.system('pip install invisible-watermark --no-deps')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "264ac4ca",
      "metadata": {
        "id": "264ac4ca"
      },
      "outputs": [],
      "source": [
        "import typing\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from diffusers import AutoPipelineForText2Image, AutoencoderKL\n",
        "from diffusers.utils import make_image_grid, pt_to_pil\n",
        "from PIL import Image, ImageFilter\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "import cv2\n",
        "\n",
        "from IPython.display import display\n",
        "from imwatermark import WatermarkEncoder, WatermarkDecoder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd946b67",
      "metadata": {},
      "source": [
        "# 1. Apply & Verify Watermarks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d8599b5",
      "metadata": {
        "id": "1d8599b5"
      },
      "source": [
        "## 1.1 Generating Images Using Stable Diffusion XL Turbo\n",
        "\n",
        "To demonstrate watermarking techniques for generated images, we will first generate an image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6269b4e6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688,
          "referenced_widgets": [
            "b6f0a47a573548bab0cd14e9862788d8",
            "3d81a61413bc4b2f86d55c53cc380fbf",
            "d55ca8feea184957be190cc8ce660495",
            "365d2f265c844420a50a2af0d0b7e205",
            "73e3b685f58145e6ac6c7555f4b10083",
            "e3b9af6ee9ae401e84d5291b48384814",
            "ff7ecaa5359b4235ba0860fc44f99671",
            "df43560cca3b49e6a940cbe2801146fa",
            "450870ff59394e3d93ea43cffb277ebe",
            "463711288643403a8eb77d66569905fa",
            "aa9510638e6f4e06beda4d424b24ad5b"
          ]
        },
        "id": "6269b4e6",
        "outputId": "0d6d9629-c3ea-489b-8ff7-1af86cd2d00c"
      },
      "outputs": [],
      "source": [
        "# load model\n",
        "pipeline = AutoPipelineForText2Image.from_pretrained(\n",
        "    \"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\"\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # performance tweaks\n",
        "    if torch.cuda.is_available():\n",
        "        pipeline.enable_sequential_cpu_offload()\n",
        "    pipeline.upcast_vae()\n",
        "\n",
        "    # generate\n",
        "    prompt = \"cute rabbit in a spacesuit\"  # Your prompt here\n",
        "    img = pipeline(prompt=prompt, guidance_scale=0.0, num_inference_steps=1).images[0]\n",
        "\n",
        "# if we are GPU poor, we load download an image instead\n",
        "else:\n",
        "    os.system(\"wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1Buwm_O774OieAbrPchfLrf7qNOgNE26C' -O rabbit.png\")\n",
        "    img = Image.open(\"rabbit.png\")\n",
        "\n",
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uRf_Vacb_bpK",
      "metadata": {
        "id": "uRf_Vacb_bpK"
      },
      "source": [
        "## 1.2 Apply a Simple Watermark to a Generated Image\n",
        "\n",
        "We now apply a DWT–DCT–SVD Watermark to the image. This scheme can encode bits of information. In out case, this is just a random bit sequence. The following cells contain everything you need to embed this watermark.\n",
        "\n",
        "You can read more about how these kind of watermarks work here, but it is not necessary for completing the tutorial:\n",
        "https://github.com/arooshiverma/Image-Watermarking-using-DCT?utm_source=chatgpt.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jXip9QBj_0bR",
      "metadata": {
        "id": "jXip9QBj_0bR"
      },
      "outputs": [],
      "source": [
        "# Watermark bits (32 bits from original constant)\n",
        "WATERMARK_MESSAGE = '1001010010011110011010011010100111101010000111000101010101000101'  # 64 bits\n",
        "WATERMARKING_SCHEME = 'dwtDctSvd'  # \"dwtDct\"\n",
        "\n",
        "def apply_watermark(cover_img: Image,\n",
        "                    watermark_message: str,\n",
        "                    method: str = WATERMARKING_SCHEME) -> Image:\n",
        "    \"\"\"\n",
        "    This function applies a simple watermark to the cover image.\n",
        "\n",
        "    @param cover_img: PIL.Image, the image to which the watermark will be applied\n",
        "    @param watermark_bits: str, the bits to be encoded in the image\n",
        "    @param method: str, the watermarking method to use\n",
        "    @return: PIL.Image, the watermarked image\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert cover image from PIL.Image to cv2, because the encoder expects BGR\n",
        "    bgr = cv2.cvtColor(np.array(cover_img), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # encode bits to cover image\n",
        "    encoder = WatermarkEncoder()\n",
        "    encoder.set_watermark('bits', watermark_message.encode('utf-8'))\n",
        "    bgr_encoded = encoder.encode(bgr, method)\n",
        "\n",
        "    # bgr_encoded to PIL\n",
        "    rgb_encoded = cv2.cvtColor(bgr_encoded, cv2.COLOR_BGR2RGB)\n",
        "    watermarked_img = Image.fromarray(rgb_encoded)\n",
        "\n",
        "    return watermarked_img"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xH5WYeHBBxAl",
      "metadata": {
        "id": "xH5WYeHBBxAl"
      },
      "source": [
        "We will now use the above code to apply a watermark to img and show the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VEMClfFeAeng",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VEMClfFeAeng",
        "outputId": "8ee9ce6a-d811-408c-b518-f1bda217052e"
      },
      "outputs": [],
      "source": [
        "watermarked_img = apply_watermark(img, WATERMARK_MESSAGE)\n",
        "\n",
        "def difference_magnified(img1: Image, img2: Image, factor=20) -> Image:\n",
        "    # get the absolute difference in pixel values between both images\n",
        "    difference = np.array(img1).astype(np.int32) - np.array(img2).astype(np.int32)\n",
        "    # absolute, do not care if negative or positive\n",
        "    difference = np.abs(difference)\n",
        "    # magnify the difference by factor\n",
        "    magnified = difference * factor\n",
        "    # clip to fit into pixel range [0, 255]\n",
        "    magnified = np.clip(magnified, 0, 255)\n",
        "    # set correct datatype\n",
        "    magnified = magnified.astype(np.uint8)\n",
        "    # to PIL Image\n",
        "    magnified = Image.fromarray(magnified)\n",
        "    return magnified\n",
        "\n",
        "display(img,\n",
        "        watermarked_img,\n",
        "        difference_magnified(img, watermarked_img))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aP8Hp-cTM6Lx",
      "metadata": {
        "id": "aP8Hp-cTM6Lx"
      },
      "source": [
        "## 1.3 Extracting a Watermark from the Image\n",
        "\n",
        "We will now extract the watermark. Each watermarking scheme provides a detector which is sensitive to the embedded watermark singal. With multi-bit watermarks like DWT-DCT-SVD, this involves extracting the encoded bits from the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "spdiRpHLzOOn",
      "metadata": {
        "id": "spdiRpHLzOOn"
      },
      "outputs": [],
      "source": [
        "def decode_watermark_message(watermarked_img: Image,\n",
        "                             watermark_message_length: int = len(WATERMARK_MESSAGE),\n",
        "                             method: str = WATERMARKING_SCHEME) -> str:\n",
        "    \"\"\"\n",
        "    This function extracts a watermark from a watermarked image.\n",
        "\n",
        "    @param watermarked_img: PIL.Image, the image from which the watermark will be extracted\n",
        "    @param watermark_message_length: int, the length of the watermark message\n",
        "    @param method: str, the watermarking method to use\n",
        "    @return: the extracted watermark bits (str)\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert to OpenCV BGR\n",
        "    bgr = cv2.cvtColor(np.array(watermarked_img), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Decode watermark\n",
        "    decoder = WatermarkDecoder('bits', watermark_message_length)\n",
        "    decoded_bits = decoder.decode(bgr, method)\n",
        "\n",
        "    # Convert both to bit strings\n",
        "    decoded_message = ''.join(f'{b:1b}' for b in decoded_bits)\n",
        "\n",
        "    return decoded_message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eP4dae8ANCIJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eP4dae8ANCIJ",
        "outputId": "dbe5b74c-d2a3-4b34-e743-24a22a588a35"
      },
      "outputs": [],
      "source": [
        "decoded_message = decode_watermark_message(watermarked_img)\n",
        "print('decoded :', decoded_message)\n",
        "print('original:', WATERMARK_MESSAGE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kAHBDd9lfQsh",
      "metadata": {
        "id": "kAHBDd9lfQsh"
      },
      "source": [
        "## 1.4 Detecting/Verifying a Watermark from the Extracted Information\n",
        "\n",
        "If an image has not been not watermarked, we would expect the number of matching bit to be 50% by pure chance. For watermarked images, the number of correct bits is much higher. We can simply set a threshold to decide if an image is watermarked with our WATERMARK_MESSAGE: If the number of correct bits after recovery is bigger than this threshold, we say it watermarked. This threshold can be theoretically calculated by applying the complementary binomial CDF for a specific target of false positives. We will not go into detail and just fix the threshold at 1 in a thousand un-watermarked images will be falsely recognized as watermarked. The corresponding threshold is 37 / 64 = 0.578125."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ji-BVTCnhBqr",
      "metadata": {
        "id": "Ji-BVTCnhBqr"
      },
      "outputs": [],
      "source": [
        "DETECTION_THRESHOLD = 0.578125\n",
        "\n",
        "def verify_watermark(original_message: str,\n",
        "                     decoded_message: str) -> typing.Union[float, bool]:\n",
        "    \"\"\"\n",
        "    Verify/detect the watermark by comparing matches and\n",
        "    checking against the detection threshold.\n",
        "\n",
        "    @param original_message: str, the original message\n",
        "    @param decoded_message: str, the decoded message\n",
        "    @return: matching_bits_ratio (int), verify_successful (bool)\n",
        "    \"\"\"\n",
        "\n",
        "    assert len(original_message) == len(decoded_message)\n",
        "\n",
        "    matches = sum(a == b for a, b in zip(original_message, decoded_message))\n",
        "    matching_bits_ratio = matches / len(original_message)\n",
        "\n",
        "    return matching_bits_ratio, matching_bits_ratio > DETECTION_THRESHOLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QTL9N1GMdqKS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTL9N1GMdqKS",
        "outputId": "280dec0a-044a-44be-842c-dc9965826475"
      },
      "outputs": [],
      "source": [
        "matching_bits_ratio, detection_success = verify_watermark(WATERMARK_MESSAGE, decoded_message)\n",
        "print('the image is',\n",
        "      'watermarked' if detection_success else \"not watermarked\",\n",
        "      'because the ratio of matching bits',\n",
        "      matching_bits_ratio,\n",
        "      'is',\n",
        "      'bigger than' if detection_success else 'smaller or equal to',\n",
        "      'the detection threshold',\n",
        "      DETECTION_THRESHOLD)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a5724f2",
      "metadata": {},
      "source": [
        "# 2. Watermark Robustness"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lAHu72LN4sSA",
      "metadata": {
        "id": "lAHu72LN4sSA"
      },
      "source": [
        "# 2.1 Watermark Removal with Common Transformations\n",
        "\n",
        "Watermarks should be robust against transformations. Ideally, an tampering with the image that is able to remove the watermark should also destroy the image content. Here are some transformation functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "shQcVMYl5Gkw",
      "metadata": {
        "id": "shQcVMYl5Gkw"
      },
      "outputs": [],
      "source": [
        "def rotate(img: Image, degrees: int) -> Image:\n",
        "    \"\"\"\n",
        "    @param img: PIL.Image, the image to be rotated\n",
        "    @param degrees: int, the degrees to rotate the image\n",
        "    @return: PIL.Image, the rotated image\n",
        "    \"\"\"\n",
        "    return transforms.RandomRotation((degrees, degrees))(img)\n",
        "\n",
        "def crop_scale(img: Image, ratio: int) -> Image:\n",
        "    \"\"\"\n",
        "    @param img: PIL.Image, the image to be cropped\n",
        "    @param ratio: int, the ratio to crop the image\n",
        "    @return: PIL.Image, the cropped image\n",
        "    \"\"\"\n",
        "    return transforms.RandomResizedCrop(img.size,\n",
        "                                        scale=(ratio, ratio),\n",
        "                                        ratio=(1, 1))(img)\n",
        "\n",
        "def random_crop(img: Image, ratio: int) -> Image:\n",
        "    \"\"\"\n",
        "    @param img: PIL.Image, the image to be cropped\n",
        "    @param ratio: int, the ratio to crop the image\n",
        "    @return: PIL.Image, the cropped image\n",
        "    \"\"\"\n",
        "    width, height, c = np.array(img).shape\n",
        "    img = np.array(img)\n",
        "    new_width = int(width * ratio)\n",
        "    new_height = int(height * ratio)\n",
        "    start_x = np.random.randint(0, width - new_width + 1)\n",
        "    start_y = np.random.randint(0, height - new_height + 1)\n",
        "    end_x = start_x + new_width\n",
        "    end_y = start_y + new_height\n",
        "    padded_image = np.zeros_like(img)\n",
        "    padded_image[start_y:end_y, start_x:end_x] = img[start_y:end_y, start_x:end_x]\n",
        "    img = Image.fromarray(padded_image)\n",
        "    return img\n",
        "\n",
        "def random_drop(img: Image, ratio: int) -> Image:\n",
        "    \"\"\"\n",
        "    @param img: PIL.Image, the image to be cropped\n",
        "    @param ratio: int, the ratio to crop the image\n",
        "    @return: PIL.Image, the cropped image\n",
        "    \"\"\"\n",
        "    width, height, c = np.array(img).shape\n",
        "    img = np.array(img)\n",
        "    new_width = int(width * ratio)\n",
        "    new_height = int(height * ratio)\n",
        "    start_x = np.random.randint(0, width - new_width + 1)\n",
        "    start_y = np.random.randint(0, height - new_height + 1)\n",
        "    padded_image = np.zeros_like(img[start_y:start_y + new_height, start_x:start_x + new_width])\n",
        "    img[start_y:start_y + new_height, start_x:start_x + new_width] = padded_image\n",
        "    img = Image.fromarray(img)\n",
        "    return img\n",
        "\n",
        "def median_blur_k(img: Image, radius: int) -> Image:\n",
        "    \"\"\"\n",
        "    @param img: PIL.Image, the image to be blurred\n",
        "    @param radius: int, the radius of the blur\n",
        "    @return: PIL.Image, the blurred image\n",
        "    \"\"\"\n",
        "    return img.filter(ImageFilter.MedianFilter(radius))\n",
        "\n",
        "def gaussian_blur(img: Image, radius: int) -> Image:\n",
        "    \"\"\"\n",
        "    @param img: PIL.Image, the image to be blurred\n",
        "    @param radius: int, the radius of the blur\n",
        "    @return: PIL.Image, the blurred image\n",
        "    \"\"\"\n",
        "    return img.filter(ImageFilter.GaussianBlur(radius=radius))\n",
        "\n",
        "def gaussian_std(img: Image, std: int) -> Image:\n",
        "    \"\"\"\n",
        "    @param img: PIL.Image, the image to be blurred\n",
        "    @param std: int, the std of the blur\n",
        "    @return: PIL.Image, the blurred image\n",
        "    \"\"\"\n",
        "    img_tensor = transforms.ToTensor()(img)\n",
        "    g_noise = torch.randn_like(img_tensor) * std\n",
        "    noisy_img_tensor = torch.clamp(img_tensor + g_noise, 0, 1)\n",
        "    img = transforms.ToPILImage()(noisy_img_tensor)\n",
        "    return img\n",
        "\n",
        "def brightness_factor(img: Image, factor: int) -> Image:\n",
        "    \"\"\"\n",
        "    @param img: PIL.Image, the image to be brightened\n",
        "    @param factor: int, the factor to brighten the image\n",
        "    @return: PIL.Image, the brightened image\n",
        "    \"\"\"\n",
        "    return transforms.ColorJitter(brightness=factor)(img)\n",
        "\n",
        "def resize(img: Image, ratio: int) -> Image:\n",
        "    \"\"\"\n",
        "    @param img: PIL.Image, the image to be resized\n",
        "    @param ratio: int, the ratio to resize the image\n",
        "    @return: PIL.Image, the resized image\n",
        "    \"\"\"\n",
        "    img_shape = np.array(img).shape\n",
        "    resize_size = int(img_shape[0] * ratio)\n",
        "    img = transforms.Resize(size=resize_size)(img)\n",
        "    img = transforms.Resize(size=img_shape[0])(img)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gqZrZcBNHQVV",
      "metadata": {
        "id": "gqZrZcBNHQVV"
      },
      "source": [
        "Let's apply the transformations and see what they do to the watermark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A6b3_kNcDlsO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "A6b3_kNcDlsO",
        "outputId": "d4b42a63-549c-4db6-83c2-29c48c109bbc"
      },
      "outputs": [],
      "source": [
        "TRANSFORMATIONS = [rotate, crop_scale, random_crop, random_drop, median_blur_k, gaussian_blur, gaussian_std, brightness_factor, resize]\n",
        "VALUES          = [75,     0.95,       0.6,         0.6,         5,             5,             0.1,          6,                 0.2]\n",
        "\n",
        "# transform the watermarked image\n",
        "def yield_transformation(img_to_be_transformed: Image):\n",
        "    for transformation, value in zip(TRANSFORMATIONS,\n",
        "                                     VALUES):\n",
        "        yield transformation(img_to_be_transformed, value)\n",
        "transformed_images = list(yield_transformation(watermarked_img))\n",
        "\n",
        "# verify the watermark in each image\n",
        "def yield_detection_metrics(images):\n",
        "    for img in images:\n",
        "        decoded_message = decode_watermark_message(img)\n",
        "        matching_bit_ratio, detection_success = verify_watermark(WATERMARK_MESSAGE, decoded_message)\n",
        "        yield matching_bit_ratio, detection_success\n",
        "matching_bit_ratios, detection_successes = zip(*list(yield_detection_metrics(transformed_images)))\n",
        "\n",
        "# display\n",
        "for transformation, value, transformed_img, matching_bit_ratio, detection_success in zip(TRANSFORMATIONS, VALUES, transformed_images, matching_bit_ratios, detection_successes):\n",
        "    print(f\"{transformation.__name__} ({value}): matching bit ratio: {matching_bit_ratio}, detection success: {detection_success}\")\n",
        "    display(transformed_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fD71h2KKKlMU",
      "metadata": {
        "id": "fD71h2KKKlMU"
      },
      "source": [
        "You can also chain tranformations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HCuhJc6LKnmA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "HCuhJc6LKnmA",
        "outputId": "26f6eab2-3ccb-478c-dc56-b083f20a2f61"
      },
      "outputs": [],
      "source": [
        "transformed_image = gaussian_std(\n",
        "    median_blur_k(\n",
        "        random_drop(\n",
        "            watermarked_img,\n",
        "            0.2),\n",
        "        7),\n",
        "    0.1)\n",
        "\n",
        "decoded_message = decode_watermark_message(transformed_image)\n",
        "matching_bit_ratio, detection_success = verify_watermark(WATERMARK_MESSAGE, decoded_message)\n",
        "\n",
        "print(f\"matching bit ratio: {matching_bit_ratio}, detection success: {detection_success}\")\n",
        "display(transformed_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pGC82IUD9yK2",
      "metadata": {
        "id": "pGC82IUD9yK2"
      },
      "source": [
        "## 2.2 Evaluating Robustness with Quality Metrics\n",
        "\n",
        "When performing watermark removal as an adversary, the goal is to remove the watermark while preserving as much of the original iamge as possible by some metric. One such metric is the peak signal-to-noise ratio (https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio), PSNR in short. We caluclate the PSNR for all the transformations from before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OglPjV14-jSY",
      "metadata": {
        "id": "OglPjV14-jSY"
      },
      "outputs": [],
      "source": [
        "def compute_psnr(img1: Image, img2: Image) -> float:\n",
        "    \"\"\"\n",
        "    Compute the peak signal-to-noise ratio (PSNR) between two images.\n",
        "\n",
        "    @param img1: PIL.Image, the first image\n",
        "    @param img2: PIL.Image, the second image\n",
        "    @return: float, the PSNR value\n",
        "    \"\"\"\n",
        "\n",
        "    img1 = np.array(img1).astype(np.float64) / 255.0\n",
        "    img2 = np.array(img2).astype(np.float64) / 255.0\n",
        "    # The PSNR is bascially just MSE measures in decibels\n",
        "    mse = np.mean((img1 - img2) ** 2)\n",
        "    # PSNR for perfectly matching images is infinity. We just set some high value instead\n",
        "    if mse == 0:\n",
        "        return 60\n",
        "    max_pixel = 1.0  # Max pixel value after normalization\n",
        "    psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
        "    return psnr\n",
        "\n",
        "\n",
        "# calculate the PSNR betweent he original watermarked image and its transfomrations\n",
        "psnr_values = []\n",
        "for transformation, value, transformed_img, matching_bit_ratio, detection_success in zip(TRANSFORMATIONS, VALUES, transformed_images, matching_bit_ratios, detection_successes):\n",
        "    # compute the PSNR between transformed_img and watermarked_img\n",
        "    psnr = compute_psnr(watermarked_img, transformed_img)\n",
        "    psnr_values.append(psnr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29rq_GQpDtsc",
      "metadata": {
        "id": "29rq_GQpDtsc"
      },
      "source": [
        "We can plot the resulting trade-off between image quality and watermark removal success like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cfa269d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "0cfa269d",
        "outputId": "3d28ed49-e726-4342-cb7b-8a453913b0f2"
      },
      "outputs": [],
      "source": [
        "def plot_robustness(transformation_names: typing.List[str],\n",
        "                    matching_bit_ratios: typing.List[float],\n",
        "                    psnr_values: typing.List[float],\n",
        "                    filename: str = None):\n",
        "    \"\"\"\n",
        "    Plots the watermark robustness based on PSNR and matching bit ratios.\n",
        "\n",
        "    @param psnr_values: list, the PSNR values for each transformation\n",
        "    @param transformation_names: list, names of the transformations applied (as strings)\n",
        "    @param matching_bit_ratios: list, the matching bit ratios for each transformation\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(psnr_values, matching_bit_ratios)\n",
        "\n",
        "    # annotate each point\n",
        "    for i, name in enumerate(transformation_names):\n",
        "        plt.annotate(name, (psnr_values[i], matching_bit_ratios[i]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "\n",
        "    # plot the detection threshold\n",
        "    plt.axhline(y=DETECTION_THRESHOLD, color='r', linestyle='--', label=f'Detection Threshold ({DETECTION_THRESHOLD})')\n",
        "    plt.legend()\n",
        "\n",
        "    # finalize the plot\n",
        "    plt.xlabel('PSNR')\n",
        "    plt.ylabel('Matching Bit Ratio')\n",
        "    plt.title('Watermark Robustness: PSNR vs Matching Bit Ratio')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    if filename is not None:\n",
        "        plt.savefig(filename)\n",
        "\n",
        "# plot\n",
        "transformation_names = [f\"{t.__name__} ({v})\" for t, v in zip(TRANSFORMATIONS, VALUES)]\n",
        "plot_robustness(transformation_names, matching_bit_ratios, psnr_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fVFlFIEaE_47",
      "metadata": {
        "id": "fVFlFIEaE_47"
      },
      "source": [
        "## 2.3 Watermark Removal Challenge\n",
        "\n",
        "Now it is your turn. Try to remove the watermor form the image while keeping PSNR above 24. You can easily check the result with the function provided below. This is not easy! Feel free to ask for hints.\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y054nRpqZnFV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y054nRpqZnFV",
        "outputId": "0648dbc5-a0cb-491d-ee13-7837f2d54582"
      },
      "outputs": [],
      "source": [
        "# When you are ready, just put the resulting image into this function\n",
        "# and show us the result\n",
        "def check_result(watermarked_img: Image, cleaned_img: Image):\n",
        "    \"\"\"\n",
        "    @param watermarked_img: PIL.Image, the watermarked image\n",
        "    @param cleaned_img: PIL.Image, the cleaned image\n",
        "    \"\"\"\n",
        "    decoded_message = decode_watermark_message(cleaned_img)\n",
        "    matching_bits_ratio, detection_success = verify_watermark(WATERMARK_MESSAGE,\n",
        "                                                              decoded_message)\n",
        "    psnr = compute_psnr(watermarked_img, cleaned_img)\n",
        "\n",
        "    print(f\"Matching bit ratio: {matching_bits_ratio}, Detection success: {detection_success}, psnr: {psnr}\")\n",
        "    display(watermarked_img, cleaned_img, difference_magnified(watermarked_img,\n",
        "                                                               cleaned_img,\n",
        "                                                               factor=1))\n",
        "\n",
        "check_result(watermarked_img,\n",
        "             transformed_images[0]  # dummy image\n",
        "             )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "waves",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "365d2f265c844420a50a2af0d0b7e205": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_463711288643403a8eb77d66569905fa",
            "placeholder": "​",
            "style": "IPY_MODEL_aa9510638e6f4e06beda4d424b24ad5b",
            "value": " 7/7 [00:03&lt;00:00,  1.59it/s]"
          }
        },
        "3d81a61413bc4b2f86d55c53cc380fbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3b9af6ee9ae401e84d5291b48384814",
            "placeholder": "​",
            "style": "IPY_MODEL_ff7ecaa5359b4235ba0860fc44f99671",
            "value": "Loading pipeline components...: 100%"
          }
        },
        "450870ff59394e3d93ea43cffb277ebe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "463711288643403a8eb77d66569905fa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73e3b685f58145e6ac6c7555f4b10083": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa9510638e6f4e06beda4d424b24ad5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6f0a47a573548bab0cd14e9862788d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d81a61413bc4b2f86d55c53cc380fbf",
              "IPY_MODEL_d55ca8feea184957be190cc8ce660495",
              "IPY_MODEL_365d2f265c844420a50a2af0d0b7e205"
            ],
            "layout": "IPY_MODEL_73e3b685f58145e6ac6c7555f4b10083"
          }
        },
        "d55ca8feea184957be190cc8ce660495": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df43560cca3b49e6a940cbe2801146fa",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_450870ff59394e3d93ea43cffb277ebe",
            "value": 7
          }
        },
        "df43560cca3b49e6a940cbe2801146fa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3b9af6ee9ae401e84d5291b48384814": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff7ecaa5359b4235ba0860fc44f99671": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
