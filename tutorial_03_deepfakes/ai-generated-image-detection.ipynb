{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b255c5",
   "metadata": {},
   "source": [
    "# AI-Generated Image Detection\n",
    "\n",
    "Generative artificial intelligence (GenAI) has evolved rapidly within the last few years.\n",
    "A central achievement is the ability to generate highly-realistic images from a simple text prompt.\n",
    "While this technology can support us in productive and creative tasks, it also holds great potential for misuse.\n",
    "AI-generated images are increasingly being used to commit fraud and spread harmful disinformation.\n",
    "\n",
    "In this assignment, we will take a closer look at the internals of a generative model and create our own images.\n",
    "Then, we will exploit the model's architecture to detect AI-generated images using [AEROBLADE](https://arxiv.org/abs/2401.17879).\n",
    "\n",
    "**Note:** If your local setup does not have a GPU, it is highly recommended to run this notebook on Google Colab (see this repository's README for instructions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fe2511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are running this notebook on Google Colab, uncomment and run the following commands\n",
    "\n",
    "# !pip install lpips --no-deps\n",
    "# !mkdir data\n",
    "# !wget https://github.com/aisoc-lab/ai-security-summer-school/blob/6df293b9cb46bddda293237af74b592457219c88/tutorial_03_deepfakes/data/real_elephant.jpg?raw=True -O data/real_elephant.jpg\n",
    "# !wget https://github.com/aisoc-lab/ai-security-summer-school/blob/6df293b9cb46bddda293237af74b592457219c88/tutorial_03_deepfakes/data/fake_elephant.png?raw=True -O data/fake_elephant.png\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d802f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lpips\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "from diffusers.utils import make_image_grid, pt_to_pil\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import to_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dd5808",
   "metadata": {},
   "source": [
    "## Generating Images Using Stable Diffusion XL Turbo\n",
    "\n",
    "The seminal work by [Rombach et al.](https://arxiv.org/abs/2112.10752), which sparked the development of [Stable Diffusion](https://github.com/CompVis/stable-diffusion) marks a milestone in text-to-image synthesis.\n",
    "In this assignment, we will be working with its successor [Stable Diffusion XL Turbo](https://arxiv.org/abs/2307.01952), which allows for the generation of highly-realistic images within a single step.\n",
    "\n",
    "A convenient way to use diffusion models is the [diffusers](https://huggingface.co/docs/diffusers/index) library.\n",
    "In the following, we create a *pipeline* from the pre-trained SDXL-Turbo model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AutoPipelineForText2Image.from_pretrained(\n",
    "    \"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\"\n",
    ")\n",
    "\n",
    "# performance tweaks\n",
    "pipeline.enable_sequential_cpu_offload()\n",
    "pipeline.upcast_vae()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6891f37e",
   "metadata": {},
   "source": [
    "We can now take a look at the model's architecture by simply printing the pipeline.\n",
    "It mostly resembles that of the original Stable Diffusion and features a U-Net, a text encoder, and a variational autoencoder (VAE).\n",
    "The VAE is used to transform an image to the latent space, in which the actual generation takes place, and back.\n",
    "It will be important later, since it can be leveraged to determine whether an image is real or AI-generated.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://github.com/aisoc-lab/ai-security-summer-school/blob/6df293b9cb46bddda293237af74b592457219c88/tutorial_03_deepfakes/data/stable-diffusion-architecture.png?raw=true\" alt=\"Architecture of Stable Diffusion\" width=\"600\"/>\n",
    "\n",
    "*Source: [https://arxiv.org/abs/2112.10752](https://arxiv.org/abs/2112.10752).*\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dd082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04bb95a",
   "metadata": {},
   "source": [
    "With diffusers, generating an image is straightforward: call the pipeline with your prompt and some additional parameters and wait for the result.\n",
    "Enter a prompt below and generate some images!\n",
    "While the guidance scale should be set to 0.0, you can vary the number of inference steps.\n",
    "Values of 2, 3, or 4 should yield better image quality at the cost of longer runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516d21ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = # Your prompt here\n",
    "img = pipeline(prompt=prompt, guidance_scale=0.0, num_inference_steps=1).images[0]\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c400bca",
   "metadata": {},
   "source": [
    "# Using the VAE\n",
    "\n",
    "We can now use the VAE to transform our generated image into the model's latent space.\n",
    "To do so, we need to first apply some pre-processing.\n",
    "Then, we can call the VAE's `encode` method on our image.\n",
    "Since a VAE outputs a distribution, we need to sample from it to obtain the actual latents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600ee30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # avoid gradient computation\n",
    "    preprocessed_img = pipeline.image_processor.preprocess(img)\n",
    "    latents = pipeline.vae.encode(preprocessed_img).latent_dist.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514353bb",
   "metadata": {},
   "source": [
    "You can now have a look at the latents.\n",
    "\n",
    "First, inspect their dimensions.\n",
    "How do they differ from those of the image?\n",
    "\n",
    "Second, visualize the latents. You can make use of the `plot_tensor` function defined below. Since the channels in the latent space do not directly map to color channels, you should plot each channel independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0919dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tensor(tensor: torch.Tensor, vmin: float | None = None, vmax: float | None = None) -> None:\n",
    "    \"\"\"\n",
    "    Plot a tensor using matplotlib's imshow. Optionally resize tensor and change minimum/maximum value for better visualization.\n",
    "    \"\"\"\n",
    "    # move channel dimension to the end and normalize\n",
    "    if tensor.ndim == 3 and tensor.shape[0] == 3:\n",
    "        tensor = tensor.permute(1, 2, 0)\n",
    "    plt.imshow(tensor.numpy(), vmin=vmin, vmax=vmax)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d5ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f589433",
   "metadata": {},
   "source": [
    "Using the VAE's decoder, we can now reconstructe the original image from the latents.\n",
    "Try this on your own. Basically, you need to reverse the steps we took to encode an image, so first call the VAE's `decode` method and then apply post-processing using the `postprocess` method of the `image_processor`.\n",
    "\n",
    "**Note:** Due to model internals, you need to change the data type of the VAE to `float32` before calling its decode method. This can be done using `pipeline.vae.to(dtype=torch.float32)`. Moreover, the `decode` function returns a data structure. You can access the actual sample using `.sample`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc1ca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc66c3dd",
   "metadata": {},
   "source": [
    "While the original image and its reconstruction look identical, they are not.\n",
    "Since the latent space is significantly smaller than the image space (to speed up generation), it cannot preserve every detail of an image.\n",
    "\n",
    "To show this, compute the absolute error between original and reconstruction and visualize the average error across all color channels.\n",
    "The `to_tensor` function might come in handy here.\n",
    "\n",
    "Have a look at what parts of the image can be reconstructed well, and what parts suffer from larger reconstruction errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff5ecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15c6f83",
   "metadata": {},
   "source": [
    "## Detecting AI-Generated Images Using AEROBLADE\n",
    "\n",
    "The previous observation is the core idea of AEROBLADE.\n",
    "It exploits the fact that generated images can be reconstructed relatively well.\n",
    "For each generated image, there is a distinct location in the VAE's latent space, which can be seen as its origin.\n",
    "\n",
    "In contrast, real images do not have an exact origin.\n",
    "They are mapped to the closest point in the latent space.\n",
    "Due to this shift, they exhibit a higher reconstruction error.\n",
    "\n",
    "Thus, if an image can be reconstructed well using a model's VAE, chances are high that it is generated by this particular model.\n",
    "\n",
    "The following figure further illustrates this idea.\n",
    "Note that we can use different distance/error metrics for $d$.\n",
    "Experiments have shown that [LPIPS](https://arxiv.org/abs/1801.03924) performs particularly well.\n",
    "<center>\n",
    "<img src=\"https://github.com/aisoc-lab/ai-security-summer-school/blob/6df293b9cb46bddda293237af74b592457219c88/tutorial_03_deepfakes/data/aeroblade.png?raw=true\" alt=\"Concept behind AEROBLADE\" width=\"600\"/>\n",
    "\n",
    "*Source: [https://arxiv.org/abs/2401.17879](https://arxiv.org/abs/2401.17879).*\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfda8c1",
   "metadata": {},
   "source": [
    "In `data/real_elephant.jpg` and `data/fake_elephant.png` you find two images we can use to demonstrate AEROBLADE.\n",
    "For both images, apply the steps from above to create their reconstructions.\n",
    "Then, use the LPIPS metric to compute the distance and visualize it.\n",
    "\n",
    "You should initialize LPIPS using `lpips_fn = lpips.LPIPS(net=\"vgg\", spatial=True)` and call it with `lpips_fn(..., normalize=True)`.\n",
    "\n",
    "**Hints:**\n",
    "- You can use `Image.open(\"path/to/file\")` to load images with PIL.\n",
    "- LPIPS expects inputs to be 4-dimensional (samples x channels x width x height).\n",
    "- Using the `vmin`/`vmax` arguments of `plot_tensor` can improve visualization, especially to align the color bars for both images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbeb676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a904d85",
   "metadata": {},
   "source": [
    "As you can see, the reconstruction error for the real image is significantly larger than that of the fake image.\n",
    "By taking the spatial average over all pixels, we obtain a single value for simple threshold-based detection of AI-generated images!\n",
    "\n",
    "**Congratulations, you made it to the end of the notebook!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aisec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
