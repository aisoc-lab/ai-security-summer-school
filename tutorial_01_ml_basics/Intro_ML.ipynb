{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09001dbf",
   "metadata": {},
   "source": [
    "### In this tutorial, you will perform both regression and classification tasks using [scikit-learn](https://scikit-learn.org/stable/getting_started.html). Before starting, make sure you have set up your conda environment according to the instructions provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0639c0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Set plot font size for better visibility\n",
    "font = {'size': 16}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f01e24",
   "metadata": {},
   "source": [
    "### ---------------------\n",
    "### 1. Generate synthetic data\n",
    "### ---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c364f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hdd_data(N, noise=False):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for hard disk drive (HDD) failure.\n",
    "    The breaking chance increases linearly with age.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # for reproducibility\n",
    "    hdd_age = np.linspace(0, 40, N).reshape(-1, 1)  # ages from 0 to 40\n",
    "    breaking_chance = 2.4 * hdd_age + 2  # linear relation\n",
    "\n",
    "    if noise:\n",
    "        measurement_noise = np.random.rand(N, 1) * 15  # add some noise\n",
    "        breaking_chance = np.minimum(100, breaking_chance + measurement_noise)\n",
    "\n",
    "    return hdd_age, breaking_chance\n",
    "\n",
    "# Generate data\n",
    "N = 40\n",
    "X, y = generate_hdd_data(N, noise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa76c1e3",
   "metadata": {},
   "source": [
    "### ---------------------\n",
    "### 2. Visualize the data\n",
    "### ---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5926d5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X, y, label=\"Data points\")\n",
    "plt.title(\"Synthetic HDD Data\")\n",
    "plt.xlabel(\"HDD Age\")\n",
    "plt.ylabel(\"Breaking Chance\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b56dbc8",
   "metadata": {},
   "source": [
    "### ---------------------\n",
    "### 3. Split data into train and test sets\n",
    "### ---------------------\n",
    "\n",
    "#### Why Cross-Validation?\n",
    "\n",
    "- When you split into train/test once, the evaluation depends on that random split.\n",
    "- Cross-validation splits the data into *k* folds (e.g. 5), trains on 4 folds and tests on 1, rotating through all folds.\n",
    "- You get multiple evaluations, giving you a better estimate of how your model will perform on unseen data.\n",
    "- Especially important for small datasets.\n",
    "\n",
    "---\n",
    "\n",
    "####  Do we need Cross-Validation for Regression?\n",
    "\n",
    "**Yes!** Any time you train any model (classification or regression), cross-validation helps:\n",
    "\n",
    "- Avoid overfitting\n",
    "- Provide more reliable performance metrics\n",
    "- Detect whether your model works well generally, or just on one lucky split\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584681d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0031fe3a",
   "metadata": {},
   "source": [
    "### ---------------------\n",
    "### 4. Train linear regression model\n",
    "### ---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c50867",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41be7998",
   "metadata": {},
   "source": [
    "### ---------------------\n",
    "### 5. Evaluate the model\n",
    "### ---------------------\n",
    "\n",
    "#### How to evaluate Regression?\n",
    "\n",
    "The most common metrics:\n",
    "\n",
    "##### MSE (Mean Squared Error)\n",
    "\n",
    "- Penalizes large errors more.\n",
    "  \n",
    "MSE = (1 / N) * sum ( (y_i - y_hat_i)^2 )\n",
    "\n",
    "\n",
    "##### MAE (Mean Absolute Error)\n",
    "\n",
    "- More robust to outliers.\n",
    "\n",
    "MAE = (1 / N) * sum ( |y_i - y_hat_i| )\n",
    "\n",
    "##### R² (Coefficient of Determination)\n",
    "\n",
    "- Proportion of variance explained by the model.\n",
    "- Values close to 1 are better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5d7af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "def evaluate(y_true, y_pred, dataset=\"\"):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{dataset} MSE: {mse:.2f}, MAE: {mae:.2f}, R2: {r2:.2f}\")\n",
    "\n",
    "evaluate(y_train, y_train_pred, \"Train\")\n",
    "evaluate(y_test, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccef2343",
   "metadata": {},
   "source": [
    "### Exercise: Predicting Fuel Efficiency with Linear Regression\n",
    "\n",
    "**Problem Description**\n",
    "You are given real-world data on automobile fuel efficiency. Each record contains information about a car’s technical specifications and its fuel efficiency measured in miles per gallon (MPG).\n",
    "In this exercise, you will build a linear regression model to predict a car’s fuel efficiency (MPG) based on its engine displacement (displacement).\n",
    "\n",
    "#### Your Tasks\n",
    "1. Load the dataset using fetch_openml:\n",
    "\n",
    "    ```python\n",
    "    from sklearn.datasets import fetch_openml\n",
    "    data = fetch_openml(\"autoMpg\", version=1, as_frame=True)\n",
    "    df = data.frame\n",
    "    ```\n",
    "    \n",
    "2. Extract:\n",
    "- Features (X): use only the displacement column.\n",
    "- Target (y): use the target values provided by data.target.\n",
    "3. Visualize the data:\n",
    "Create a scatter plot showing displacement (engine size) vs MPG (fuel efficiency).\n",
    "4. Split the data into train and test sets:\n",
    "Use 80% of the data for training and 20% for testing.\n",
    "5️. Train a Linear Regression model to predict MPG from displacement.\n",
    "6. Evaluate your model using:\n",
    "- Mean Squared Error (MSE)\n",
    "- Mean Absolute Error (MAE)\n",
    "- R² score\n",
    "\n",
    "7️. Visualize the model predictions:\n",
    "- Plot the test data points.\n",
    "- Plot the predicted values from your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac68f83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_openml(\"autoMpg\", version=1, as_frame=True)\n",
    "df = data.frame\n",
    "\n",
    "# X = displacement column (feature)\n",
    "X = df[['displacement']].values\n",
    "\n",
    "# y = target (mpg) directly from data.target\n",
    "y = data.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b47817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_openml(\"autoMpg\", version=1, as_frame=True)\n",
    "df = data.frame\n",
    "\n",
    "# Select X and y correctly\n",
    "X = df[['displacement']].values  # feature\n",
    "y = data.target.values  # target\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X, y, alpha=0.7)\n",
    "plt.xlabel(\"Engine Displacement (cubic inches)\")\n",
    "plt.ylabel(\"Fuel Efficiency (MPG)\")\n",
    "plt.title(\"Engine Size vs Fuel Efficiency\")\n",
    "plt.show()\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"R² Score: {r2:.3f}\")\n",
    "\n",
    "# Visualize predictions\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_test, y_test, label=\"Test data\", alpha=0.7)\n",
    "plt.scatter(X_test, y_pred, color='red', label=\"Predictions\", s=50)\n",
    "plt.xlabel(\"Engine Displacement (cubic inches)\")\n",
    "plt.ylabel(\"Fuel Efficiency (MPG)\")\n",
    "plt.title(\"Linear Regression: Predicting Fuel Efficiency\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de33f86-b7d6-4729-ba07-a6cfe6af1194",
   "metadata": {},
   "source": [
    "# Training Classifiers: Logistic Regression & Neural Network\n",
    "\n",
    "In this part, we will:\n",
    "\n",
    "- Generate synthetic classification data using `scikit-learn`.\n",
    "- Visualize the data.\n",
    "- Train two classifiers: \n",
    "    - Logistic Regression\n",
    "    - Multi-Layer Perceptron (Neural Network)\n",
    "- Visualize decision boundaries of both models.\n",
    "\n",
    "---\n",
    "\n",
    "> We will use `make_classification` to create 2D data that we can easily plot and analyze.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841cf9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f032eaf-9673-48d8-b83d-652a62d85269",
   "metadata": {},
   "source": [
    "## Generate and Visualize Dataset\n",
    "\n",
    "We generate a 2D binary classification dataset using `make_classification`.\n",
    "\n",
    "- `n_samples`: number of data points.\n",
    "- `n_features`: number of features (we use 2 for visualization).\n",
    "- `n_informative`: number of informative features.\n",
    "- `class_sep`: controls how easily separable the classes are.\n",
    "- `random_state`: for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90977cb1-099c-4783-9e59-e4f722cd401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "x, y = make_classification(\n",
    "    n_samples=500, \n",
    "    n_features=2, \n",
    "    n_informative=2, \n",
    "    n_redundant=0, \n",
    "    n_classes=2,\n",
    "    class_sep=0.6, \n",
    "    random_state=1123344,\n",
    ")\n",
    "\n",
    "# Visualize dataset\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x[y==0, 0], x[y==0, 1], label='Class 0', alpha=0.7)\n",
    "plt.scatter(x[y==1, 0], x[y==1, 1], label='Class 1', alpha=0.7)\n",
    "plt.title(\"Generated 2D classification data\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e60548",
   "metadata": {},
   "source": [
    "## Training a model and drawing the decision boundary\n",
    "\n",
    "We will now:\n",
    "\n",
    "- Train a Logistic Regression model.\n",
    "- Use `meshgrid` and `contourf` to draw its decision boundary.\n",
    "- Then repeat the same for `MLPClassifier` (Neural Network).\n",
    "- Finally, we will compare how linear vs nonlinear models behave.\n",
    "\n",
    "**Key idea**: Logistic Regression can only create linear boundaries, while Neural Networks can learn non-linear boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be1028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(x, y)\n",
    "\n",
    "# Predict and calculate accuracy\n",
    "y_pred_logreg = logreg.predict(x)\n",
    "acc_logreg = accuracy_score(y, y_pred_logreg)\n",
    "print(f\"Logistic Regression accuracy: {acc_logreg:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80c5d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MLP (Neural Network)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=2000, random_state=42)\n",
    "mlp.fit(x, y)\n",
    "\n",
    "# Predict and calculate accuracy\n",
    "y_pred_mlp = mlp.predict(x)\n",
    "acc_mlp = accuracy_score(y, y_pred_mlp)\n",
    "print(f\"Neural Network (MLP) accuracy: {acc_mlp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2330147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a meshgrid\n",
    "def make_grid_feature_vectors(axis_size=200):\n",
    "    x1_range = np.linspace(-5, 5, axis_size)\n",
    "    x2_range = np.linspace(-5, 5, axis_size)\n",
    "    x1_grid, x2_grid = np.meshgrid(x1_range, x2_range)\n",
    "    feature_vectors = np.c_[x1_grid.ravel(), x2_grid.ravel()]\n",
    "    return x1_range, x2_range, feature_vectors, x1_grid, x2_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726dd98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid\n",
    "x1_range, x2_range, feature_vectors, x1_grid, x2_grid = make_grid_feature_vectors()\n",
    "\n",
    "# Predict for each classifier\n",
    "z_logreg = logreg.predict(feature_vectors).reshape(x1_grid.shape)\n",
    "z_mlp = mlp.predict(feature_vectors).reshape(x1_grid.shape)\n",
    "\n",
    "# Plot Logistic Regression\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.contourf(x1_range, x2_range, z_logreg, cmap='Pastel1', alpha=0.7)\n",
    "plt.scatter(x[y==0, 0], x[y==0, 1], label='Class 0')\n",
    "plt.scatter(x[y==1, 0], x[y==1, 1], label='Class 1')\n",
    "plt.title(\"Logistic Regression Decision Boundary\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot Neural Network\n",
    "plt.subplot(1,2,2)\n",
    "plt.contourf(x1_range, x2_range, z_mlp, cmap='Pastel1', alpha=0.7)\n",
    "plt.scatter(x[y==0, 0], x[y==0, 1], label='Class 0')\n",
    "plt.scatter(x[y==1, 0], x[y==1, 1], label='Class 1')\n",
    "plt.title(\"Neural Network (MLP) Decision Boundary\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691550db",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Logistic Regression draws a straight linear boundary between classes.\n",
    "- Neural Network (MLP) can fit non-linear complex boundaries.\n",
    "- By changing parameters of `make_classification()` you can create more challenging datasets.\n",
    "\n",
    " **Your homework**: try changing:\n",
    "- `class_sep` to make classes more/less separable.\n",
    "- `n_informative` to add noise.\n",
    "- `random_state` to generate different datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64071c80",
   "metadata": {},
   "source": [
    "### Exercise: Non-linear Classification using Logistic Regression and Neural Network\n",
    "\n",
    "Problem Description: You are given a non-linear binary classification problem where two classes are shaped like interleaving moons (non-linearly separable data).\n",
    "\n",
    "Your goal is to:\n",
    "- Train two classifiers:\n",
    "- Logistic Regression\n",
    "- Neural Network (MLPClassifier)\n",
    "- Compare their performance and visualize their decision boundaries.\n",
    "\n",
    "1. Generate the dataset using make_moons() function from sklearn.datasets:\n",
    "Use n_samples=500, noise=0.2, and random_state=42.\n",
    "2. Visualize the dataset using scatter plot, color-coded by class.\n",
    "3. Train a Logistic Regression model on the full dataset and compute its accuracy.\n",
    "4. Train a Neural Network (MLPClassifier) on the same data with:\n",
    "MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=2000, random_state=42)\n",
    "and compute its accuracy.\n",
    "5. Build a meshgrid covering the input space and visualize the decision boundaries of both models side-by-side.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ba4d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate dataset\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "x, y = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
    "\n",
    "# 2. Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x[y==0, 0], x[y==0, 1], label='Class 0', alpha=0.7)\n",
    "plt.scatter(x[y==1, 0], x[y==1, 1], label='Class 1', alpha=0.7)\n",
    "plt.title(\"Moons Dataset\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 3. Train Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(x, y)\n",
    "y_pred_logreg = logreg.predict(x)\n",
    "acc_logreg = accuracy_score(y, y_pred_logreg)\n",
    "print(f\"Logistic Regression accuracy: {acc_logreg:.2f}\")\n",
    "\n",
    "# 4. Train MLPClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=2000, random_state=42)\n",
    "mlp.fit(x, y)\n",
    "y_pred_mlp = mlp.predict(x)\n",
    "acc_mlp = accuracy_score(y, y_pred_mlp)\n",
    "print(f\"Neural Network (MLP) accuracy: {acc_mlp:.2f}\")\n",
    "\n",
    "# 5. Create meshgrid and visualize decision boundaries\n",
    "import numpy as np\n",
    "\n",
    "def make_grid_feature_vectors(axis_size=200):\n",
    "    x1_range = np.linspace(-2, 3, axis_size)\n",
    "    x2_range = np.linspace(-1.5, 2, axis_size)\n",
    "    x1_grid, x2_grid = np.meshgrid(x1_range, x2_range)\n",
    "    feature_vectors = np.c_[x1_grid.ravel(), x2_grid.ravel()]\n",
    "    return x1_range, x2_range, feature_vectors, x1_grid, x2_grid\n",
    "\n",
    "x1_range, x2_range, feature_vectors, x1_grid, x2_grid = make_grid_feature_vectors()\n",
    "\n",
    "z_logreg = logreg.predict(feature_vectors).reshape(x1_grid.shape)\n",
    "z_mlp = mlp.predict(feature_vectors).reshape(x1_grid.shape)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.contourf(x1_range, x2_range, z_logreg, cmap='Pastel1', alpha=0.7)\n",
    "plt.scatter(x[y==0, 0], x[y==0, 1], label='Class 0')\n",
    "plt.scatter(x[y==1, 0], x[y==1, 1], label='Class 1')\n",
    "plt.title(\"Logistic Regression Decision Boundary\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.contourf(x1_range, x2_range, z_mlp, cmap='Pastel1', alpha=0.7)\n",
    "plt.scatter(x[y==0, 0], x[y==0, 1], label='Class 0')\n",
    "plt.scatter(x[y==1, 0], x[y==1, 1], label='Class 1')\n",
    "plt.title(\"Neural Network (MLP) Decision Boundary\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aisec)",
   "language": "python",
   "name": "aisec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
