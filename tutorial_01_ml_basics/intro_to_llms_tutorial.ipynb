{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfbb5767",
   "metadata": {},
   "source": [
    "# Evaluating LLMs\n",
    "\n",
    "In this assignment, we will examine generations of LLMs and evaluate their outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "093dfcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers torch tqdm accelerate hf-xet datasets --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "090d695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import matplotlib\n",
    "import os\n",
    "from transformers import GenerationConfig\n",
    "import datasets\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "torch.device('cpu')\n",
    "# Selecting the font size here will affect all the figures in this notebook\n",
    "# Alternatively, you can set the font size for axis labels of each figure separately\n",
    "font = {'size': 16}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae4268a",
   "metadata": {},
   "source": [
    "# Exercise 1: Measuring bias using model probabilities [45 mins]\n",
    "\n",
    "In this exercise, we will use the [CrowS Pairs data](https://github.com/nyu-mll/crows-pairs) to measure bias in the outputs of a model.\n",
    "\n",
    "The dataset consists of counterfactual statements like `Women don't know how to drive.` and `Men don't know how to drive.` and measures which statements the model agrees with more.\n",
    "\n",
    "We will measure the agreement using the probability the model assigns to an input. In other words, say the input consists of t tokens $[t_1, t_2, \\ldots, t_N]$. Then we can measure the model assigned (log) probability as: $\\log(t_2 | t_1) + \\log(t_3 | t_1, t_2) + \\ldots + \\log(t_M | t_{M-1}, \\ldots, t_1)$.\n",
    "\n",
    "The dataset consists of pairs of sentences called `sent_more` and `sent_less`. Your bias metric is _the number of times the model assigns more probability to `sent_more`._\n",
    "\n",
    "\n",
    "**Hint:** You can pass `output_logits=True` to the [GenerationConfig](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationConfig.output_logits) to return logits, which you can then convert to softmax probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24c08de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\" # Very small model with only 600M parameters\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a0cb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=10,\n",
    "    temperature=0.5,\n",
    "    seed=1,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "prompt = \"Hello\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False,\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "\n",
    "outputs = model.generate(**model_inputs, generation_config=generation_config)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277dda42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd03158",
   "metadata": {},
   "source": [
    "## Exercise 2: Comparing the two models. [40 mins]\n",
    "\n",
    "Download the [IMDB movie reviews dataset](https://huggingface.co/datasets/stanfordnlp/imdb). The inputs are the movie reviews written by users. The outputs are the sentiment of the users. The sentiment is a binary labels.\n",
    "\n",
    "\n",
    "Your task is to:\n",
    "\n",
    "1. Download the dataset using `datasets.load_dataset(\"stanfordnlp/imdb\")`.\n",
    "2. Select 50 samples with positive and 50 samples with negative sentiment.\n",
    "3. Prompt the model and compute its performance.\n",
    "4. Compare the perforamnce with a larger model `Qwen/Qwen3-4B`.\n",
    "5. Repeat the same procedure with the [dolly dataset](https://huggingface.co/datasets/databricks/databricks-dolly-15k). Limit yourself to the classification category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385107ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aisec)",
   "language": "python",
   "name": "aisec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
