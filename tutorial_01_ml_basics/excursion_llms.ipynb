{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfbb5767",
   "metadata": {},
   "source": [
    "# First contact with open source LLMs\n",
    "\n",
    "In this exericse, you will perform inference using open source LLMs with the [HuggingFace Transformers library](https://huggingface.co/docs/transformers/en/index).\n",
    "\n",
    "HuggingFace is the de-facto standard for releasing LLMs and related datasets.\n",
    "\n",
    "Make sure you have set up your conda environment following the instructions from week 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76100141-ae45-47b9-9d59-e02e8cca4fee",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "In this exercise, we will download a \"small\" LLM and see how the tokenization and inference works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e97223-18ac-4651-b4e3-9ec90863833b",
   "metadata": {},
   "source": [
    "## Exercise 1a: Downloading and preparing the model\n",
    "\n",
    "You do not have to solve anything in this exercise. You are already given the solution. Your task is to simply to walk through and understand it.\n",
    "\n",
    "We will work with a small model which consists of 500 million parameters. It is not as large and as capable as ChatGPT. But you can easily run it on your machine. In the lectures that follow, we will experiment with larger models.\n",
    "\n",
    "You can learn more about the model [here](https://huggingface.co/Qwen/Qwen2.5-0.5B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf71a81e-0f39-46be-93db-41ae75ddb349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import random\n",
    "import datasets\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "# Selecting the font size here will affect all the figures in this notebook\n",
    "# Alternatively, you can set the font size for axis labels of each figure separately\n",
    "font = {'size': 16}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a1c6b-e5fc-438e-a2e3-e54c2f353d70",
   "metadata": {},
   "source": [
    "### Initialize the model\n",
    "\n",
    "Every model on HuggingFace has a unique name. Each model also comes with its own tokenizer. You can download the model and the corresponding tokenizer using this unique name.\n",
    "\n",
    "The next cell might take a while to run. You need to download around a Gigabyte of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb96343-5f86-49e8-8b15-12cdbb057d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913da1f2-bd5b-4ad0-83f8-972daa7f887c",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "You will see how the input gets converted to tokens IDs. Each ID just represents an individual text token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e57d9-2647-4450-990e-0953e46a339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"How do you do on this splendid day?\"\n",
    "tokenized_input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "print(tokenized_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ae412d-cdd0-4651-be5c-b11ebbfafa96",
   "metadata": {},
   "source": [
    "You can ignore the attention mask. For causal LLMs, it is mostly important when processing more than one input texts at a time.\n",
    "\n",
    "Let us print the tokens that each ID represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02fddaa-7ea8-4288-9298-8f7e0fcedb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"].numpy().flatten())\n",
    "print(input_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e0133f-8d29-4508-9e4d-0202f98efe78",
   "metadata": {},
   "source": [
    "The \"Ä \" character represents a preceding space.\n",
    "\n",
    "We can also convert these tokens back to a full text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfdef14-8f2d-4db1-ae01-8ffa3ca0d70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.convert_tokens_to_string(input_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015a8743-cfd2-4669-a6d0-8da08736435e",
   "metadata": {},
   "source": [
    "### Vocabulary size\n",
    "\n",
    "Let us print the size of the model vocabulary, that is, the total number of tokens that it has seen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd586e05-f5e5-4728-854f-f8928ef81abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b1cd71-dc56-4292-a044-7801588756b3",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Recall that LLMs are really just Transformer models, which take the input and generate $V$ scores where $V$ is the total number of tokens in our vocabulary. One reasonable way to generate next token is by selecting the token with the highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24fed38-54fb-4568-959f-967527d34090",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Is Bochum a great city?\"\n",
    "\n",
    "tokenized_input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = tokenized_input[\"input_ids\"]\n",
    "print(f\"Shape of input IDs: {input_ids.shape}\")  # number of inputs x number of tokens\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(input_ids.numpy().flatten())\n",
    "with torch.no_grad():\n",
    "   output = model(tokenized_input[\"input_ids\"]).logits  # We should pass the attention mask but we can ignore it for causal LLMs when we have just a single input\n",
    "print(f\"Shape of the output: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d521be0-8bc5-4d8a-a092-b47b2b8a5614",
   "metadata": {},
   "source": [
    "As we can see, the model computes the output score for **every single input token**.\n",
    "\n",
    "Let us compute what the most likely token at each position is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6906d0c-2e3b-4ff6-96b3-d64c63cde25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_likely_tokens = output.squeeze(dim=0)  # Remove the first dimension which is 1\n",
    "most_likely_tokens = most_likely_tokens.argmax(axis=-1)  # At each generation position, select the token with the highest score\n",
    "output_tokens = tokenizer.convert_ids_to_tokens(most_likely_tokens.numpy())\n",
    "\n",
    "input_so_far = []\n",
    "next_token = []\n",
    "for i in range(len(input_tokens)):\n",
    "    input_text = tokenizer.convert_tokens_to_string(input_tokens[:i+1])  # combine all the input tokens up to this generation position\n",
    "    gen_token = tokenizer.convert_tokens_to_string([output_tokens[i]])\n",
    "    input_so_far.append(input_text)\n",
    "    next_token.append(gen_token)\n",
    "\n",
    "pd.DataFrame({\"Input\": input_so_far, \"Model output\": next_token})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c29b34-a497-4969-9781-c47d2f6bced6",
   "metadata": {},
   "source": [
    "## Exercise 1b: Generating multiple tokens\n",
    "\n",
    "Your task is to write a function that takes an input prompt and a specific generation length. It then generates as many new tokens as specified by generation length. At each position, you will generate the most likely next token.\n",
    "\n",
    "Test the function with a few prompts like:\n",
    "1. Germany is a country\n",
    "2. Abraham Lincoln was born in\n",
    "\n",
    "Feel free to add prompts of your own liking :)\n",
    "\n",
    "**Hint:** Recall that LLMs are autoregressive. That is, after generating the first token, you append it back to the input to generate the second token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87f9a50-4f14-42fb-89dd-d799369424d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt: str, gen_len: int) -> str:\n",
    "    # Your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "prompt = \"I love Bochum because\"\n",
    "generate(prompt, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded18ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt: str, gen_len: int) -> str:\n",
    "    # Your code here\n",
    "    # raise NotImplementedError\n",
    "    tokenized_input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = tokenized_input[\"input_ids\"]\n",
    "    for _ in range(gen_len):\n",
    "        with torch.no_grad():\n",
    "           output = model(input_ids).logits  # We should pass the attention mask but we can ignore it for causal LLMs when we have just a single input\n",
    "        output = output.squeeze(dim=0)\n",
    "        next_token_scores = output[-1]\n",
    "        next_token_id = next_token_scores.argmax(dim=-1)\n",
    "        input_ids = torch.cat((input_ids, torch.LongTensor([next_token_id]).reshape(1,-1)), dim=-1)\n",
    "    return tokenizer.decode(input_ids.numpy().flatten())\n",
    "\n",
    "prompt = \"I love Bochum because\"\n",
    "# generate(prompt, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd846021-c898-47ee-9b78-68c9ff2845c9",
   "metadata": {},
   "source": [
    "# Exercise 2: Stochastic generations and temperature\n",
    "\n",
    "In this exercise, we will continue with LLM generations. We will try stochastic generations and also fiddle with temperature values.\n",
    "\n",
    "Remember, in order to ensure reproducibility, we need to set our seeds before we call stochastic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68587003-2723-49cf-8e1f-ecccd28267d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dd4746-dce3-4436-a008-acbc409a23fe",
   "metadata": {},
   "source": [
    "## Exercise 2a: Generating stochastically\n",
    "\n",
    "Fill the `stochastic_generate` function that generates the model output based on the softmax distribution.\n",
    "\n",
    "Test your function on the prompts above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa2f968-7863-42fd-86fe-421e04095fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_generate(prompt: str, gen_len: int, temp: float = 1):\n",
    "    # Your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "prompt = \"Berlin is a city in\"\n",
    "stochastic_generate(prompt, 10, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6740b8-d2ab-44e0-af63-33b8d718eb5c",
   "metadata": {},
   "source": [
    "## Exercise 2b: Generating on a real world data with different temperatures\n",
    "\n",
    "Below we download the BOLD dataset for you. The dataset consists of some incomplete sentences from wikipedia which the LLMs are supposed to finish.\n",
    "\n",
    "Select 10 prmompts for this data. For each prompt, generate the outputs 5 times.\n",
    "\n",
    "Repeat the procedure for the following temperatures:\n",
    "1. T = 0.00001\n",
    "2. T= 1\n",
    "3. T =2\n",
    "\n",
    "What differences do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba289109",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc682ef-99e8-495f-83a5-3fedcf9bb402",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_prompts = 10\n",
    "bold = datasets.load_dataset(\"AlexaAI/bold\")\n",
    "random.seed(11)\n",
    "prompts = []\n",
    "for prompt_list in bold[\"train\"][\"prompts\"]:\n",
    "    prompts.extend(prompt_list)\n",
    "random.shuffle(prompts)\n",
    "prompts = prompts[:n_prompts]\n",
    "for prompt in prompts:\n",
    "    print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32836965-24dd-40b8-b74f-da391e71ba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab77571-3c44-49e4-9d1b-e130718cf2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
